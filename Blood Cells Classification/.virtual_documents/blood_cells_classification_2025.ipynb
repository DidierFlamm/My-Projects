








# Chemin d'accès aux images brutes
path_DS_raw = './images/raw'

# Taille des images après pre-process 
img_height = 100                            #les images d'origine étant presque carrées,
img_width = 100                             #le format 100x100 offre en général un bon rapport performance/rapidité
target_size = (img_height, img_width)

# Stockage des images après pre-processing et split
path_DS_train = '../../../../Downloads/pre_processed/train'
path_DS_valid = '../../../../Downloads/pre_processed/valid'
path_DS_test = '../../../../Downloads/pre_processed/test'

# Stockage des images après segmentation (binarisation)
path_DS_train_bin = '../../../../Downloads/binarized/train'
path_DS_valid_bin = '../../../../Downloads/binarized/valid'
path_DS_test_bin = '../../../../Downloads/binarized/test'





from typing import Tuple, List, Dict, Optional, Union
import time
import os
from pathlib import Path
import matplotlib.pyplot as plt
%matplotlib inline
import cv2
from PIL import Image
import numpy as np
import pandas as pd
import hashlib
from tqdm import tqdm
from sklearn import ensemble
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from xgboost import XGBClassifier
from collections import Counter
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from skimage.color import rgb2gray
from skimage.filters import threshold_otsu
from IPython.display import display





def data_viz(path: Optional[Union[str, Path]] = None, X = None, y= None)-> Dict[str, int]:
    """
    Affiche la répartition des images par classe à partir d’un répertoire ou de deux tableaux (X et y).

    Parameters
    ----------
    path : str, optional
        Chemin vers le dossier contenant les sous-dossiers (classes).
    X : array-like, optional
        Données d’image (non utilisées, seulement requis pour cohérence avec y).
    y : array-like, optional
        Étiquettes associées aux données X.

    Returns
    -------
    Dict[str, int]
        Dictionnaire des classes et de leur nombre d’images.
    """
    
    if path is None and (X is None or y is None):
        raise ValueError("Path or (X and y) are missing")

    if path is not None and X is not None and y is not None:
        raise ValueError("Can't have both path and (X and y)")

    if path is not None:
        path = Path(path)  # Conversion explicite de path en Path
    
    if X is not None and y is not None:
        # comptage des images par label de y           
        compteur = Counter(y)

    if path is not None:
        # comptage des images par catégores (= sous-dossier de directory)            
        
        compteur = {}
    
        for subdir in path.iterdir():
            if subdir.is_dir():
                compteur[subdir.name] = len(list(subdir.glob("*")))
    
    print(sum(compteur.values()), 'images found, belonging to', len(compteur.keys()), 'classes:', list(compteur.keys()))
    
    # création de 2 graphiques
    
    plt.figure(figsize=(12, 5))
    
        # graph en barre
    
    compteur = dict(sorted(compteur.items())) #tri alphabétique des clés
    
    plt.subplot(121)
    plt.bar(x=compteur.keys(), height= compteur.values())
    plt.xticks(rotation=70)
    plt.title("Répartition des images par classe");

    
        # camembert 
    
    labels_quantites = [f"{label}\n({val})" for label, val in compteur.items()]
    
    plt.subplot(122)
    plt.pie(x = compteur.values(),
          labels = labels_quantites,
          autopct = lambda x: str(round(x, 1)) + ' %',
          pctdistance = 0.7,
          labeldistance = 1.2)
    plt.title("Proportions par classe");
    
    plt.tight_layout()
    plt.show();
    
    return compteur


def load_images(path, target_size=None, drop_duplicates=True):
    X, y, names = [], [], []
    path = Path(path)

    #Load, convert to RGB, resize and convert to ndarray
    class_folders = [p for p in path.iterdir() if p.is_dir()]
    print(f"Found {len(class_folders)} classes: {[p.name for p in class_folders]}")

    for class_path in tqdm(class_folders, desc=f"Loading images from {len(class_folders)} subfolders"):
        class_name = class_path.name
        for file_path in class_path.iterdir():
            try:
                with Image.open(file_path) as img:
                    img = img.convert('RGB')
                    if target_size is not None : img = img.resize(target_size)
                    arr = np.array(img)

                    X.append(arr)
                    y.append(class_name)
                    names.append(file_path.name)
            except Exception as e:
                print(f"❌ Skipped {file_path}: {e}")

    print("✅", len(y), 'images loaded belonging to', len(set(y)), 'classes:', set(y))

    
    # print number of different sizes found
    X_sizes = set(img.shape for img in X)    
    print(f"ℹ️ {len(X_sizes)} image size(s) returned:", sorted(X_sizes))
    

    if target_size is not None:
        return np.array(X), np.array(y), np.array(names) 
    else: 
        return X,y,names



def load_images_cv2(directory: str) -> Tuple[List[np.ndarray], List[str], List[str]] :
        
    X = []
    y = []
    names = []

    subdir = [file for file in os.listdir(directory)
              if os.path.isdir(os.path.join(directory, file))]
    
    for label in tqdm(subdir, desc=f"Loading images from {len(subdir)} directories"):
        path = os.path.join(directory, label)
        for filename in os.listdir(path):
            img_path = os.path.join(path, filename)
            img = cv2.imread(img_path)
            if img is None:
                print(f"❌ Image non lisible : {img_path}")
                continue

            # Convertir BGR -> RGB car imread retourne du BGR
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            X.append(img)
            y.append(label)
            names.append(filename)

    print("✅", len(y), 'images loaded belonging to', len(set(y)), 'classes:', set(y))
    
    X_image_size =set(np.array(img).shape for img in X)
    
    print(f"ℹ️ {len(X_image_size)} taille(s) d'images chargées:", sorted(X_image_size))
    
    return X, y, names


def generate_filenames(y: List[str]) -> List[str]:
    """
    Créé des noms d'images à partir d'une liste de labels, au format label_XXX.png
    Utilisée par write_images si la liste des noms des images n'est pas fournie
    """
    dico = {}
    names = []
    for label in y:
        dico[label] = dico.get(label, 0) + 1
        names.append(f"{label}_{dico[label]:03}.png")  # 0-padding sur 3 chiffres
    return names

def hash_image(img: np.ndarray) -> str:
    """Créé un hash unique pour chaque image qui permet de détecter les doublons bcp plus rapidement: complexité O(n) au lieu de O(n2)"""
    return hashlib.sha256(img.tobytes()).hexdigest() 

def pre_process_images(X: List[np.ndarray],
                   y: List[str] = None,
                   names: List[str] = None,
                   img_width: int = 100, 
                   img_height: int = 100,
                   drop_duplicates: bool = True,
                   ) -> Tuple[np.ndarray, np.ndarray, np.ndarray] :

    X_processed = []
    
    if y is not None:
        y_processed = y.copy()
        if names is None:
            names_processed = generate_filenames(y)
        else:
            names_processed = names.copy() 
    
    

    for i, img in enumerate(tqdm(X, desc="Pre-processing images (normalization + resize + RGB conversion)")):
        img_resized = cv2.resize(img, (img_width, img_height))                  # NB : resize prend (width, height)
                                                                                
        if img_resized.ndim == 2:                                               # if the image is in grayscale (ndim = 2) => convertis en 3 canaux RGB artificiels
            img_resized = img_resized.reshape([img_height, img_width, 1])       # reshape avec un canal singleton NB: reshape prend (height, width, canal)
            img_resized = np.concatenate([img_resized] * 3, axis=2)             # then concatenate 3 times to have an artificial RGB image (with R=G=B)

        # Normalisation simple : convertir en float32 et diviser par 255
        img_resized = img_resized.astype(np.float32) / 255.0
        
        X_processed.append(img_resized)
    
    if y is not None:
        print("✅", len(X_processed), 'images processed belonging to', len(set(y_processed)), 'classes:', set(y))
    else:
        print("✅", len(X_processed), 'images processed')

    # Supprimer les doublons (optimisé grâce à l'utilisation du hash pour identifier les doublons au lieu de comparer les images pixel par pixel)
    
    if drop_duplicates:

        seen_hashes ={}
        indices_to_keep = []
        duplicates = []
        
        for i, img in enumerate(tqdm(X_processed, desc="Détection des doublons via hashing")):
            h = hash_image(img)
            if h not in seen_hashes:         #si le hash de l'image i est nouveau
                seen_hashes[h] = i           #on mémorise que le hash a été vu sur l'indice i
                indices_to_keep.append(i)    #ajout de i aux indices à garder
            else:
                previous = seen_hashes[h]           # previous = indice de l'image existante
                duplicates.append((previous, i))      # l'image i est un doublon de previous
        

        
        #affichage des doublons après la boucle
        for n, (previous,i) in enumerate(duplicates):
            if y is not None: print(f"{n+1}. {y_processed[previous]} {names_processed[previous]} est conservée mais pas son doublon {y_processed[i]} {names_processed[i]}")
            plt.subplot(121)
            plt.imshow(X_processed[previous])
            if y is not None: plt.title(y_processed[previous] + "\n" + names_processed[previous])
            plt.axis(False)
            plt.subplot(122)
            plt.imshow(X_processed[i])
            if y is not None: plt.title(y_processed[i] + "\n" + names_processed[i])
            plt.axis(False)
            plt.show();

        print("⚠️",len(duplicates), 'duplicates have been detected and removed') 


        # filtre sur les indices à garder et transforme en ndarray
        X_processed = np.array([X_processed[i] for i in indices_to_keep])
        if y is not None: 
            y_processed = np.array([y_processed[i] for i in indices_to_keep])
            names_processed = np.array([names_processed[i] for i in indices_to_keep]) 
            print("✅",len(X_processed), "images left, belonging to", len(set(y_processed)), 'classes:', set(y_processed))
        else:
            print("✅",len(X_processed), "images left")

    if y is not None:
        return X_processed,y_processed, names_processed
    else:
        return X_processed


start_time = time.perf_counter()
X,y,names = load_images(path_DS_raw)#), target_size=target_size)
print(time.perf_counter()-start_time)


start_time = time.perf_counter()
X,y,names = load_images_cv2(path_DS_raw)
X,y,names = pre_process_images(X,y,names,drop_duplicates= False)
print(time.perf_counter()-start_time)





data_viz(path = path_DS_raw)











if LOAD_RAW:
    X,y,names = load_images(path_DS_raw)


## 2. Redimensionnement des images, suppression des doublons et normalisation (données entre 0 et 1)


X_pre_processed,y_pre_processed,names_pre_processed = pre_process_images(X=X,y=y,names=names, img_height=img_height, drop_duplicates=True)





# séparation des images avec stratify=y pour conserver les proportions de chaque classe
# on split aussi names pour pouvoir les utiliser ensuite dans write_images

# 1. Train (70%) vs Temp (30%)
X_train, X_temp, y_train, y_temp, names_train, names_temp = train_test_split(
    X_pre_processed, y_pre_processed, names_pre_processed, test_size=0.3, stratify=y_pre_processed, random_state=42)

# 2. Temp (30%) → Valid (15%) + Test (15%)
X_valid, X_test, y_valid, y_test, names_valid, names_test = train_test_split(
    X_temp, y_temp, names_temp, test_size=0.5, stratify=y_temp, random_state=42)


# vérfication des proportions après split

def print_class_distribution(y : List[str], name: str = "Set") -> None:
    counts = Counter(y)
    total = sum(counts.values())
    # Trouver la longueur max des noms de classes pour l'alignement
    max_len = max(len(cls) for cls in counts)
    print(f"\n=== {name} ===\tTotal: {total}\n")
    print(f"{'Classe':<{max_len}} {'Count':>7} {'%':>7}")
    print("-" * (max_len + 16))  # longueur totale approximative du titre

    for cls, count in sorted(counts.items()):
        print(f"{cls:<{max_len}} {count:7}  {count/total:6.2%}")

print_class_distribution(y_train, "Train")
print_class_distribution(y_valid, "Valid")
print_class_distribution(y_test, "Test")





def write_images(X: List[np.ndarray],y: List[str], output_dir: str, names: List[str] = None, overwrite: bool = False) -> None :
    """
    Sauvegarde une liste d'images X dans des sous-dossiers selon leurs labels y.
    Optionnellement, utilise une liste de noms de fichiers 'names'.
    """
    
    warnings_count = 0

    # Gestion des ValueError
    if names is not None and len(names) != len(X):
        raise ValueError("Les listes 'X' et 'names' doivent avoir la même longueur")
    if len(y) != len(X):
        raise ValueError("Les listes 'X' et 'y' doivent avoir la même longueur")

    # Création de la liste names si elle n'est pas fournie
    if names is None:
        names = generate_filenames(y)
    
    # Création des sous-dossiers
    for label in set(y):
        subfolder = os.path.join(output_dir, label)
        try:
            os.makedirs(subfolder, exist_ok=True)
        except OSError as e:
            raise RuntimeError(f"Impossible de créer le dossier '{subfolder}': {e}")

    # Sauvegarde des images
    for idx, (img, label) in enumerate(tqdm(zip(X, y), total=len(X))):
        filename = names[idx]

        # Conversion en unint8 si nécessaire
        if img.max() <= 1.0:                         #si l'image est normalisée (exemple après cv2.imread)
            img = (img * 255).copy()                 #on multiplie la valeur des pixels pour avoir un entier entre 0 et 255 (et copy pour éviter de modifier X) 
        
        img = np.clip(img,0,255).astype(np.uint8)    #on s'assure dans tous les cas que que les valeurs de pixels soient au format uint8 requis par cv2.cvtColor et cv2.imwrite
                                                     # et on clip par sécurité pour s'assurer d'avoir des valeurs entre 0 et 255 (évite des éventuelles erreurs ultérieures)
        
        if img.ndim == 3 and img.shape[2] == 3:           # image couleur (ndim = 3 et la 3ème dimmension contient 3 canaux RGB)
            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)    # RGB -> BGR car X est en RGB mais imwrite prend du BGR...
        elif img.ndim == 2:                               # image niveaux de gris en 2D, OK
            pass  
        elif img.ndim == 3 and img.shape[2] == 1:         # image niveaux de gris en 3D (avec un canal singleton)
            img = img.reshape(img.shape[0], img.shape[1]) # on la reshape en 2D pour éviter un bug de imwrite
        else:
            raise ValueError(f"Format d'image inattendu : shape={img.shape}")

        
        # Ecriture de l'image
        image_path = os.path.join(output_dir, label, filename)
        if os.path.exists(image_path) and not overwrite:
            warnings_count += 1
        else:
            cv2.imwrite(image_path, img)

    if warnings_count > 0:
        print(f"⚠️ {warnings_count} fichiers existaient déjà et n'ont pas été écrasés.")

    print(f"✅ {len(X) - warnings_count} images sauvegardées dans {output_dir}")


write_images(X_train,y_train, output_dir = path_DS_train, names = names_train, overwrite= False)
write_images(X_valid,y_valid, output_dir = path_DS_valid, names = names_valid, overwrite= False)
write_images(X_test,y_test, output_dir = path_DS_test, names = names_test, overwrite= False)





#reprendre ici pour skip les 1ères étapes
RELOAD = 1
if RELOAD:
    X_train, y_train, names_train = load_images(path_DS_train)
    X_valid, y_valid, names_valid = load_images(path_DS_valid) 
    X_test, y_test, names_test = load_images(path_DS_test) 


# Reshape en 2D pour le resampling qui utilise des images applaties (1D) donc des sets en 2D (n_samples, n_features)

def flatten_set(X):
    """Aplatie un ensemble d'images ou d’échantillons en 2D (n_samples, features).
    
    Paramètres
    ----------
    X : array-like de forme (n_samples, ...)
        Ensemble d'images ou de données multidimensionnelles.
    
    Retourne
    -------
    X_flat : ndarray de forme (n_samples, n_features)
        Données aplaties pour être utilisées dans des modèles de machine learning classiques.
    """
    X = np.asarray(X)
    return X.reshape(len(X), -1)
    
X_train_2D = flatten_set(X_train)
X_valid_2D = flatten_set(X_valid)
X_test_2D = flatten_set(X_test)


from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)


# Création de l'estimateur XGBoost
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train_encoded)))



# Entraînement du modèle
model.fit(X_train_2D, y_train_encoded)

# Prédiction sur les données de test
y_pred_encoded = model.predict(X_valid_2D)

# Convertir les prédictions en format original
y_pred= label_encoder.inverse_transform(y_pred_encoded)


display(pd.crosstab(y_valid, y_pred))
print(classification_report(y_valid, y_pred))





# Sur-échantillonnage uniquement sur train : cette classe du module imbalanced-learn duplique aléatoirement des exemples de la classe minoritaire jusqu'à équilibrer les classes.
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros = ros.fit_resample(X_train_2D, y_train)


def data_viz_sets(X, y)-> Dict:

    # comptage des images par label de y           

    compteur = Counter(y)
    
    print(sum(compteur.values()), 'images found, belonging to', len(compteur.keys()), 'classes')
    
    # création de 2 graphiques
    
    plt.figure(figsize=(12, 5))
    
        # graph en barre des catégories par ordre croissant de nombre d'images
    
    compteur_inc = dict(sorted(compteur.items()))
    
    plt.subplot(121)
    plt.bar(x=compteur_inc.keys(), height= compteur_inc.values())
    plt.xticks(rotation=70);
    
        # camembert par ordre alphabétique
    
    compteur = dict(sorted(compteur.items())) #tri alphabétique des clés
    
    lq = []
    for elem in compteur.items():
        lq.append(elem[0] + "\n(" + str(elem[1]) + ")") #label avec nom et quantité
    
    plt.subplot(122)
    plt.pie(x = compteur.values(),
          labels = lq,
          autopct = lambda x: str(round(x, 1)) + ' %',
          pctdistance = 0.7,
          labeldistance = 1.2);
    
    plt.show();
    
    return compteur


data_viz(X = X_train_ros, y= y_train_ros)





# Sous-échantillonnage uniquement sur train : cette classe du module imbalanced-learn supprime aléatoirement des exemples de la classe majoritaire jusqu'à équilibrer les classes.
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train_2D, y_train)


data_viz(X = X_train_rus, y = y_train_rus)








# Sélection d'une image aléatoire
n = np.random.randint(0, len(X_train))
img = X_train[n]

# Affichage original
gray_img = rgb2gray(img)

plt.figure(figsize=(12,4))
plt.subplot(1, 4, 1)
plt.imshow(img)
plt.title("Image couleur")
plt.axis('off')

# Affichage en gray scale
plt.subplot(1, 4, 2)
plt.imshow(gray_img, cmap="gray")
plt.title("Image en niveaux de gris")
plt.axis('off')

# Affichage de l'istogramme des intensités
plt.subplot(1, 4, 3)
plt.hist(gray_img.ravel(),       #gray_img.ravel() : transforme l'image 2D en 1D pour histogramme.
         bins=256,               #bins=256 : divisions fines entre 0 et 1 (comme les niveaux de gris).
         range=(0, 1),           #range=(0, 1) : force l’échelle correcte pour les images normalisées.
         color='gray')           #color='gray' : visuel cohérent avec le contenu.

# Ajout du seuil Otsu sur l'histogramme
seuil_otsu = threshold_otsu(gray_img) # Calcul du seuil automatique Otsu
plt.axvline(seuil_otsu, color='red', linestyle='--', label=f'Seuil Otsu = {seuil_otsu:.2f}')
plt.legend()
plt.title('Histogramme et seuil Otsu')
plt.xlabel("Intensité")
plt.ylabel("Nombre de pixels")

# Affichage de l'image binarisée par Otsu
binary_otsu = gray_img > seuil_otsu # Binarisation avec ce seuil
plt.subplot(1, 4, 4)
plt.imshow(binary_otsu, cmap='gray', vmin=0, vmax=1)
plt.title('Image binarisée par Otsu')
plt.axis('off')

plt.tight_layout()
plt.show()

# Affichage de la binarisation selon différents seuils
plt.figure(figsize=(12, 10))
for i in range(10):
    threshold = i * 0.1
    binarized = gray_img > threshold
    plt.subplot(2, 5, i + 1)
    plt.imshow(binarized, cmap="gray", vmin=0, vmax=1)  # vmin et vmax sont ajoutées pour que les images toutes blanches n'apparaissent pas toute noire
    plt.title(f"Seuil > {threshold:.1f}")
    plt.axis('off')
plt.tight_layout()
plt.show()





def get_threshold_otsu(X) -> float:
    """
    Calcule le seuil Otsu global sur un ensemble d’images (RGB ou grayscale).

    Args:
        X (array-like): images RGB (H, W, 3) ou grayscale (H, W)

    Returns:
        seuil_otsu (float): seuil optimal
    """
    gray_pixels = []
    for img in X:
        if img.ndim == 3 and img.shape[2] == 3:
            gray = rgb2gray(img)
        elif img.ndim == 2:
            gray = img
        else:
            raise ValueError(f"Image de forme inattendue : {img.shape}")
        gray_pixels.append(gray.ravel())

    all_pixels = np.concatenate(gray_pixels)
    threshold = threshold_otsu(all_pixels)
    print(f"Seuil Otsu global : {threshold:.3f}")
    return threshold


otsu = get_threshold_otsu(X_train)





def apply_threshold(X: list[np.ndarray], threshold: float) -> np.ndarray:
    """
    Applique une binarisation à un ensemble d’images (RGB ou grayscale) avec un seuil donné.
    Convertit les images RGB en niveaux de gris si besoin,
    puis binarise en 0/1 selon threshold.
    
    Retourne un np.ndarray de forme (n_images, height, width) dtype float (0 ou 1).
    """
    binarized_images = []
    
    for img in X:
        # Standardiser si besoin et convertir en float au cas où (et ne pas modifier l'original)
        img_f = img.astype(np.float32) / 255 if img.dtype == np.uint8 else img.astype(np.float32)
        
        # Conversion en niveaux de gris si nécessaire
        if img_f.ndim == 3 and img_f.shape[2] == 3:
            gray = rgb2gray(img_f)
        elif img_f.ndim == 2:
            gray = img_f
        else:
            raise ValueError(f"Image de forme inattendue : {img.shape}")
        
        # Binarisation : pixels > threshold deviennent 1, sinon 0
        binary = (gray > threshold).astype(float)
        
        binarized_images.append(binary)
    
    return np.array(binarized_images)



X_train_bin = apply_threshold(X_train, otsu)
X_valid_bin = apply_threshold(X_valid, otsu)
X_test_bin  = apply_threshold(X_test, otsu)





write_images(X_train_bin, y_train, output_dir= path_DS_train_bin, names=names_train, overwrite= False)


#applatissement des sets pour être utilisés dans les modèles de ML    
X_train_bin = flatten_set(X_train_bin)
X_valid_bin = flatten_set(X_valid_bin)
X_test_bin = flatten_set(X_test_bin)





def get_ML_acc(clf, X_train, X_valid, y_train, y_valid, flatten : bool = True, normalization : bool = False, standardization: bool = False, label_encoder_fitted = None, verbose : bool = False) -> float:
    
    """
    Fonction d’évaluation pour modèles classiques de ML (SVM, KNN, RF, XGBoost).
    Aplatie automatiquement les images.
    Ne convient pas pour modèles deep learning.

    Parameters
    ----------
    clf : Classifieur scikit-learn-compatible
    X_train, X_valid : array-like
        Données d'entraînement et de validation (images)
    y_train, y_valid : array-like
        Labels correspondants
    flatten : bool (True par défaut)
        Applique un reshape en 2D des ensembles X_train et X_valid
    normalization : bool (False par défaut)
        Applique un MinMaxScaler
    standardization : bool (False par défaut)
        Applique un StandardScaler
    label_encoder_fitted
        Encode y_train et y_valid avec le label encoder déjà entrainés
    verbose : bool (False par défaut)
        Affiche accuracy_score, la matrice de confusion croisée et le classification_report

    Returns
    -------
    float : accuracy_score
    """

    if label_encoder_fitted is not None:
        y_train = label_encoder_fitted.transform(y_train)
        #y_valid = label_encoder_fitted.transform(y_valid)
    
    if flatten:
        X_train_2D = np.asarray(X_train).reshape(len(X_train), -1)
        X_valid_2D = np.asarray(X_valid).reshape(len(X_valid), -1)
    else:
        X_train_2D = np.asarray(X_train)
        X_valid_2D = np.asarray(X_valid)

    if normalization and standardization:
        raise ValueError("Can't have both Normalization and Standardization")
    
    if normalization:                                     # données dans [0, 1] # exemple : KNN
        scaler = MinMaxScaler()
        X_train_2D = scaler.fit_transform(X_train_2D)
        X_valid_2D = scaler.transform(X_valid_2D)

    if standardization:                                   # données centrées réduites (moyenne = 0, std = 1) # exemple : SVM
        scaler = StandardScaler()
        X_train_2D = scaler.fit_transform(X_train_2D)
        X_valid_2D = scaler.transform(X_valid_2D)
        
    if verbose: 
        print("Processing fit and predict...")
        start_time = time.perf_counter()       # start timing prediction

    clf.fit(X_train_2D, y_train)
    
    y_pred = clf.predict(X_valid_2D)

    if label_encoder_fitted is not None:
            # Décoder y_valid et y_pred pour affichage lisible
            #y_valid_decoded = label_encoder_fitted.inverse_transform(y_valid)
        y_pred = label_encoder_fitted.inverse_transform(y_pred)
        #else:
        #    y_valid_decoded = y_valid
        #    y_pred_decoded = y_pred
    
    accuracy = accuracy_score(y_valid, y_pred)

    if verbose:
        print('accuracy:', accuracy)
        end_time = time.perf_counter()         # end timing
        predict_time = end_time - start_time   # durée en secondes
        print(f'prediction duration: {predict_time:.3f} seconds')
        

        
        display(pd.crosstab(y_valid, y_pred))
        print(classification_report(y_valid, y_pred))

    return accuracy
    

def get_4_ML_acc(X_train, X_valid, y_train, y_valid, label_encoder_fitted = None, verbose : bool = False, n_jobs:int = -1) -> pd.DataFrame:

    # Flatten une seule fois ici puis flatten = False sur les 4 appels suivant pour gagner en perf...
    X_train_2D = np.asarray(X_train).reshape(len(X_train), -1)
    X_valid_2D = np.asarray(X_valid).reshape(len(X_valid), -1)
    
    #RF
    clf = ensemble.RandomForestClassifier(n_jobs=n_jobs)
    print("=== Random Forest ===")
    start_time = time.perf_counter()
    acc_RF = get_ML_acc(clf, X_train_2D, X_valid_2D, y_train, y_valid, flatten = False, normalization = False, standardization = False, label_encoder_fitted = label_encoder_fitted, verbose = verbose)
    end_time = time.perf_counter()         
    predict_time = end_time - start_time   
    duration_RF = round(predict_time,3)
    
    #SVM
    #standardisation des données recommandées
    clf = SVC(kernel="rbf", C=1, gamma='scale') # le paramètre n_jobs n'existe pas dans SVC 
    print("=== Support Vector Machine ===")
    start_time = time.perf_counter()
    acc_SVM = get_ML_acc(clf, X_train_2D, X_valid_2D, y_train, y_valid, flatten = False, normalization = False, standardization = True, label_encoder_fitted = label_encoder_fitted, verbose = verbose)
    end_time = time.perf_counter()         
    predict_time = end_time - start_time   
    duration_SVM = round(predict_time,3)
    
    #KNN
    #normalisation des données recommandées
    clf = KNeighborsClassifier(n_neighbors=5, n_jobs = n_jobs)
    start_time = time.perf_counter()
    print("=== k-Nearest Neighbors ===")
    acc_KNN = get_ML_acc(clf, X_train_2D, X_valid_2D, y_train, y_valid, flatten = False, normalization = True, standardization = False, label_encoder_fitted = label_encoder_fitted, verbose = verbose)
    end_time = time.perf_counter()         
    predict_time = end_time - start_time   
    duration_KNN = round(predict_time,3)
    
    #XGB désactivé car plante
    acc_XGB = "XGB?"
    duration_XGB = "?XGB"

    """
    #XGB
    clf = XGBClassifier(
        num_class=len(np.unique(y_train)),   # nombre de classes
        learning_rate=0.1,                   # taux d'apprentissage
        max_depth=6,                         # profondeur max des arbres, évite le sur-apprentissage
        eval_metric='mlogloss',              # métrique pour classification multi-classe
        n_jobs = n_jobs)
    start_time = time.perf_counter()
    print("=== XGBoost ===")
    acc_XGB = get_ML_acc(clf, X_train_2D, X_valid_2D, y_train, y_valid, flatten = False, normalization = False, standardization = False, label_encoder_fitted = label_encoder_fitted, verbose = verbose)
    end_time = time.perf_counter()         
    predict_time = end_time - start_time   
    duration_XGB = round(predict_time,3)"""

    
    
    #Bilan
    
    df = pd.DataFrame({
        "Classifier": ["Random Forest", "SVM", "KNN", "XGBoost"],
        "Accuracy": [acc_RF, acc_SVM, acc_KNN, acc_XGB],
        "Duration (s)": [duration_RF, duration_SVM, duration_KNN, duration_XGB]
    }).set_index("Classifier")
    display(df)
    

    return df
    








# Création d'une instance de RandomForestClassifier,
# n_jobs=-1 signifie que le calcul doit être effectué en parallèle sur tous les cœurs de l'ordinateur
clf = ensemble.RandomForestClassifier(n_jobs=-1)
# Entraînement du modèle RandomForestClassifier sur le jeu de données d'entraînement
clf.fit(X_train_2D, y_train)
# Prédiction des étiquettes pour le jeu de données valid
y_pred = clf.predict(X_valid_2D)


pd.crosstab(y_valid, y_pred) # Affichage de la matrice de confusion entre les étiquettes réelles du jeu de données de test et les étiquettes prédites


# Affichage du rapport de classification
cr_rf_pp = classification_report(y_true=y_valid, y_pred=y_pred)
print(cr_rf_pp)


### Test de la fonction sur pre_processed
RF_acc_pre_processed = get_ML_acc(clf, X_train, X_valid, y_train, y_valid, verbose = True)


### Test de la fonction sur bin
RF_acc_bin = get_ML_acc(clf, X_train_bin, X_valid_bin, y_train, y_valid, verbose = True)


### Test de la fonction sur rus
RF_acc_rus = get_ML_acc(clf, X_train_rus, X_valid, y_train_rus, y_valid, verbose = True)


### Test de la fonction sur ros
RF_acc_ros = get_ML_acc(clf, X_train_ros, X_valid, y_train_ros, y_valid, verbose = True)


# Test de 4_ML sans XGB mais avec LE
# pre_processed
le = LabelEncoder()
le.fit(y_train)
acc_pp = get_4_ML_acc(X_train, X_valid, y_train, y_valid, label_encoder_fitted = le, verbose = True)


# Test de 4_ML sans XGB mais avec LE
# BIN
le = LabelEncoder()
le.fit(y_train)
acc_bin = get_4_ML_acc(X_train_bin, X_valid_bin, y_train, y_valid, label_encoder_fitted = le, verbose = True)


# Test de 4_ML sans XGB mais avec LE
# RUS
le = LabelEncoder()
le.fit(y_train_rus)
acc_rus = get_4_ML_acc(X_train_rus, X_valid, y_train_rus, y_valid, label_encoder_fitted = le, verbose = True)


# Test de 4_ML sans XGB mais avec LE
# ROS
le = LabelEncoder()
le.fit(y_train_ros)
acc_ros = get_4_ML_acc(X_train_ros, X_valid, y_train_ros, y_valid, label_encoder_fitted = le, verbose = True)


# Test de XGBoost
le = LabelEncoder()
le.fit(y_train) # je réassigne sinon ça plante ?!
clf = XGBClassifier(
        num_class=len(np.unique(y_train)),   # nombre de classes
        learning_rate=0.1,                   # taux d'apprentissage
        max_depth=6,                         # profondeur max des arbres, évite le sur-apprentissage
        eval_metric='mlogloss',              # métrique pour classification multi-classe
        n_jobs = -1)

score_XGB = get_ML_acc(clf, X_train, X_valid, y_train, y_valid, flatten = True, normalization = False, standardization = False, label_encoder_fitted = le, verbose = True)


# Test minimal de XGB avec PCA

from sklearn.decomposition import PCA
pca = PCA(n_components=200)  # à ajuster

le = LabelEncoder()
y_train_enc=le.fit_transform(y_train)
y_valid_enc=le.transform(y_valid)
#y_test_enc = le.transform(y_test)


X_train_pca = pca.fit_transform(X_train_2D)
X_valid_pca = pca.transform(X_valid_2D)
#X_test_pca = pca.fit_transform(X_test_2D)

#classique
#clf = XGBClassifier(eval_metric='mlogloss')

#force le CPU
clf = XGBClassifier(eval_metric='mlogloss', tree_method='hist')

#force le GPU
#clf = XGBClassifier(eval_metric='mlogloss', tree_method='gpu_hist')

clf.fit(X_train_pca, y_train_enc)

y_pred_enc = clf.predict(X_valid_pca)
score = accuracy_score(y_valid_enc, y_pred_enc)
print("Accuracy :", score)

"""
clf = XGBClassifier(
        n_estimators=10,
        max_depth=2,
        eval_metric='mlogloss'
        )
clf.fit(X_valid_2D, y_valid_enc)
y_pred_enc = clf.predict(X_test_2D)
score = accuracy_score(y_test_enc, y_pred_enc)
print("Accuracy :", score)
"""


# Test de 4_ML
le = LabelEncoder()
le.fit(y_train)
acc_pre_processed = get_4_ML_acc(X_train, X_valid, y_train, y_valid, label_encoder_fitted = le, verbose = True)





# Test minimal de XGBoost

from sklearn.datasets import load_iris
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
import pandas as pd

data = load_iris()
X = data.data
y = data.target

# Séparation en jeu d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entraînement
clf = XGBClassifier(eval_metric='mlogloss')
clf.fit(X_train, y_train)

# Prédiction
y_pred = clf.predict(X_test)

# Évaluation
score = accuracy_score(y_test, y_pred)
print("Accuracy :", score)

print(pd.crosstab(y_test, y_pred))
print(classification_report(y_test, y_pred))








# Entraînement et évaluation du modèle sur les données oversamplées
clf = ensemble.RandomForestClassifier(n_jobs=-1) 
clf.fit(X_train_ros, y_train_ros)
y_pred_ros = clf.predict(X_valid_2D)
cr_rf_ros = classification_report(y_valid, y_pred_ros)
print(cr_rf_ros)





# Entraînement et évaluation du modèle sur les données undersamplées
clf = ensemble.RandomForestClassifier(n_jobs=-1) 
clf.fit(X_train_rus, y_train_rus)
y_pred_rus = clf.predict(X_valid_2D)
cr_rf_rus = classification_report(y_valid, y_pred_rus)
print(cr_rf_rus)





clf = ensemble.RandomForestClassifier(n_jobs=-1) 
clf.fit(X_train_bin, y_train)
y_pred_bin = clf.predict(X_valid_bin)
cr_rf_bin = classification_report(y_valid, y_pred_bin)
print(cr_rf_bin)








from sklearn.svm import SVC
# Initialisation du classificateur SVC avec un noyau linéaire et une fonction de décision 'one vs one'
cls = SVC(kernel='linear', decision_function_shape='ovo')
# Entraînement du modèle SVM sur les données d'entraînement
cls.fit(X_train, y_train)




# Utilisation du modèle entraîné pour prédire les étiquettes de classe pour les données de test
y_pred = cls.predict(X_test)

# Génération d'une matrice de confusion pour montrer le nombre de prédictions correctes et incorrectes, classées par classe
pd.crosstab(y_test, y_pred)





# Importation de la fonction classification_report pour obtenir des statistiques détaillées sur la performance du modèle
from  sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))





score = cls.score(X_test, y_test)
print(score)





# Importation du module SVM (Support Vector Machine) de sklearn
from sklearn.svm import SVC

# Initialisation du classificateur SVC avec un noyau linéaire et une fonction de décision 'one vs one'
cls = SVC(kernel='linear', decision_function_shape='ovo')

# Entraînement du modèle SVM sur les données d'entraînement
cls.fit(X_seg_train_2D, y_train)





# Utilisation du modèle entraîné pour prédire les étiquettes de classe pour les données de test
Y_pred = cls.predict(X_seg_test_2D)

# Génération d'une matrice de confusion pour montrer le nombre de prédictions correctes et incorrectes, classées par classe
pd.crosstab(Y_test, Y_pred)



#
print(classification_report(Y_test, Y_pred))




# Calcul et affichage du score du modèle sur les données de test
score = cls.score(X_seg_test_2D, Y_test)
print(score)








from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Réorganiser les dimensions des données d'images en une seule dimension
X2D = np.reshape(X, (len(X), -1))
print('shape de X2D:', X2D.shape)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X2D, Y, test_size=0.2)

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_train_encoded = label_encoder.fit_transform(Y_train)

# Création de l'estimateur XGBoost
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

# Entraînement du modèle
model.fit(X_train, Y_train_encoded)

# Prédiction sur les données de test
Y_pred_encoded = model.predict(X_test)

# Convertir les prédictions en format original
Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

# Affichage de la matrice de confusion
pd.crosstab(Y_test, Y_pred_xg)








# Affichage du rapport de classification
print(classification_report(Y_test, Y_pred_xg))





X_seg_train, X_seg_test, Y_train, Y_test = train_test_split(X_seg,Y,test_size=0.2)
# Conversion de la liste X_seg_train en un tableau numpy pour une manipulation plus facile
X_seg_train = np.asarray(X_seg_train)

# Affichage de la forme du tableau X_seg_train pour vérifier sa dimension
print(X_seg_train.shape)

# Conversion de la liste X_seg_test en un tableau numpy pour une manipulation plus facile
X_seg_test = np.asarray(X_seg_test)

# Affichage de la forme du tableau X_seg_test pour vérifier sa dimension
print(X_seg_test.shape)

# Affichage de la forme de Y pour vérifier sa dimension. Y est le vecteur des étiquettes de classe.
print(Y.shape)

# Aplatissement des images segmentées
X_seg_train_2D = np.reshape(X_seg_train, (len(X_seg_train), -1))
X_seg_test_2D = np.reshape(X_seg_test, (len(X_seg_test), -1))

print(X_seg_train_2D .shape)
print(X_seg_test_2D .shape)

print(X_seg_train.shape)
print(X_seg_test.shape)
print(Y.shape)


from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_train_encoded = label_encoder.fit_transform(Y_train)

# Création de l'estimateur XGBoost
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

# Entraînement du modèle
model.fit(X_seg_train_2D, Y_train_encoded)

# Prédiction sur les données de test
Y_pred_encoded = model.predict(X_seg_test_2D)

# Convertir les prédictions en format original
Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

# Affichage de la matrice de confusion
pd.crosstab(Y_test, Y_pred_xg)



from sklearn.metrics import classification_report

# Affichage du rapport de classification
print(classification_report(Y_test, Y_pred_xg))






#Random Forest





# Importation des bibliothèques nécessaires
from  sklearn.metrics import accuracy_score
from sklearn import ensemble
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Spécification du nombre d'itérations pour la validation Hold-Out
num_hold= 10
# Initialisation d'une liste vide pour stocker les précisions à chaque itération
accuracies= [ ]

# Redimensionnement de X en 2D pour l'adapter au modèle
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)

# Boucle sur le nombre d'itérations de validation Hold-Out
for i in range(num_hold):
    # Séparation des données en ensembles d'entraînement et de test
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)

    # Création et entraînement du modèle RandomForestClassifier
    clf = ensemble.RandomForestClassifier(n_jobs=-1)
    clf.fit(X_train, Y_train)

    # Prédiction des classes pour l'ensemble de test
    Y_pred_r = clf.predict(X_test)

    # Calcul de la précision pour cette itération
    accuracy =accuracy_score(Y_test, Y_pred_r )
    # Ajout de la précision à la liste des précisions
    accuracies.append(accuracy)

    # Affichage de la précision pour cette itération
    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de la précision à travers les itérations
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
# Ajout d'une ligne horizontale représentant la précision moyenne sur toutes les itérations
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')
# Ajout des labels pour les axes et le titre du graphique
plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
# Affichage du graphique
plt.show()

# Affichage des précisions pour chaque itération
print(f" Accuracy avec la validation Hold-Out: {accuracies}")

# Affichage de la précision moyenne sur toutes les itérations
print(f" Accuracy avec la validation Hold-Out: {np.mean(accuracies)}")



#SVM





# Importation des bibliothèques nécessaires
from  sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

num_hold= 10 # Définition du nombre d'itérations pour la validation Hold-Out
accuracies= [ ] # Initialisation d'une liste vide pour stocker les scores d'accuracy
# Redimensionnement de X en 2D pour s'adapter au modèle SVM
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)


# Boucle sur le nombre d'itérations de validation Hold-Out
for i in range(num_hold):
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)
     # Création et entraînement du modèle SVC
    cls = SVC(kernel='linear', decision_function_shape='ovo')
    cls.fit(X_train, Y_train)
    Y_pred_s = cls.predict(X_test) # Prédiction des classes pour l'ensemble de test

    accuracy =accuracy_score(Y_test, Y_pred_s ) # Calcul de l'accuracy pour cette itération
    accuracies.append(accuracy) # Ajout de l'accuracy à la liste des accuracies

    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de l'accuracy
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')

plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
plt.legend()
plt.show()



print(f" Accuracy avec la validation Hold-Out: {accuracies}")
print(f" Accuracy moyenne avec la validation Hold-Out: {np.mean(accuracies)}")

#3H pour 10 itérations






from  sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np

num_hold= 10
accuracies= [ ]
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)

for i in range(num_hold):
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)

    # Convertir les classes en format numérique
    label_encoder = LabelEncoder()
    Y_train_encoded = label_encoder.fit_transform(Y_train)

    # Création de l'estimateur XGBoost
    model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

    # Entraînement du modèle
    model.fit(X_train, Y_train_encoded)

    # Prédiction sur les données de test
    Y_pred_encoded = model.predict(X_test)

    # Convertir les prédictions en format original
    Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

    accuracy =accuracy_score(Y_test, Y_pred_xg)
    accuracies.append(accuracy)

    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de l'accuracy
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')

plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
plt.legend()
plt.show()


print(f" Accuracy avec la validation Hold-Out: {accuracies}")
print(f" Accuracy moyenne avec la validation Hold-Out: {np.mean(accuracies)}")


#15 h et 36 min pour 9 itérartions et plante
#5h 28 min avec plus d'unités
"""shape de X2D: (17204, 30000)
Iteration 1, Accuracy avec la validation Hold-Out: 0.9154315605928509
Iteration 2, Accuracy avec la validation Hold-Out: 0.9067131647776809
Iteration 3, Accuracy avec la validation Hold-Out: 0.8959604766056379
Iteration 4, Accuracy avec la validation Hold-Out: 0.9038070328392909
Iteration 5, Accuracy avec la validation Hold-Out: 0.8936355710549259
Iteration 6, Accuracy avec la validation Hold-Out: 0.902353966870096
Iteration 7, Accuracy avec la validation Hold-Out: 0.9102005231037489
Iteration 8, Accuracy avec la validation Hold-Out: 0.9084568439407149
Iteration 9, Accuracy avec la validation Hold-Out: 0.9072943911653589
Iteration 10, Accuracy avec la validation Hold-Out: 0.8977041557686719

 Accuracy avec la validation Hold-Out: [0.9154315605928509, 0.9067131647776809, 0.8959604766056379, 0.9038070328392909, 0.8936355710549259, 0.902353966870096, 0.9102005231037489, 0.9084568439407149, 0.9072943911653589, 0.8977041557686719]
 Accuracy moyenne avec la validation Hold-Out: 0.9041557686718976 """"








from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, KFold
import numpy as np

# Remodeler X en une matrice 2D
# K-Fold

"""n_samples = X.shape[0]
n_features = np.prod(X.shape[1:])
X_2D = X.reshape(n_samples, n_features)"""

# Redimensionnement de nos données en 2 dimensions si nécessaire
X2D = np.reshape(X,(len(X),-1))

# Définition du modèle
model = RandomForestClassifier(n_jobs=-1)

# Définition du K-Fold Cross Validation
kfold = KFold(n_splits=10, shuffle=True)

# Calcul des scores via cross validation
scores_kfold = cross_val_score(model, X2D, Y, cv=kfold)

print(f"Scores avec K-Fold Cross Validation: {scores_kfold}")
print(f"Moyenne des scores K-Fold: {np.mean(scores_kfold)}")








"""
import xgboost as xgb
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Redimensionnement de nos données en 2 dimensions si nécessaire
X2D = np.reshape(X,(len(X),-1))

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)

# Définition du modèle
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_encoded)), n_jobs=-1)

# Définition du K-Fold Cross Validation
kfold = KFold(n_splits=4, shuffle=True)

# Calcul des scores via cross validation
scores_kfold = cross_val_score(model, X2D, Y_encoded, cv=kfold)

print(f"Scores avec K-Fold Cross Validation: {scores_kfold}")
print(f"Moyenne des scores K-Fold: {np.mean(scores_kfold)}")

# nécessite plusieurs RAM """











def DidDataGen(directory_train, directory_test, target_size = (100,100), batch_size = 32, shear_range = 0.2, zoom_range = 0.2,
                rotation_range = 359, horizontal_flip = True, vertical_flip = True):

  from tensorflow.keras.preprocessing.image import ImageDataGenerator
  from tensorflow.keras.applications.vgg16 import preprocess_input
  import os

  train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                    shear_range = shear_range, # plage d'étirement
                                    zoom_range = zoom_range,  # plage d'agrandissement
                                    rotation_range = rotation_range, # plage de rotation en degré
                                    horizontal_flip = horizontal_flip,  #retournement horizontal aléatoire
                                    vertical_flip = vertical_flip,  #retournement vertical aléatoire
                                    )

  test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)

  train_generator = train_datagen.flow_from_directory(directory = directory_train,
                                                            class_mode ="sparse",
                                                            target_size=target_size,
                                                            batch_size=batch_size)

  test_generator = test_datagen.flow_from_directory(directory = directory_test,
                                                          class_mode ="sparse",
                                                          target_size=target_size,
                                                          batch_size=batch_size)

    # compte le nb de sous-dossiers dans directory_train
  n_class=0
  for file in os.listdir(directory_train):
      d = os.path.join(directory_train, file)
      if os.path.isdir(d):
          n_class += 1

  return train_generator, test_generator, n_class


train_generator, test_generator, n_class = DidDataGen(path_DS_train, path_DS_test, target_size = (100,100), batch_size = 32, shear_range = 0.2, zoom_range = 0.2,
                rotation_range = 359, horizontal_flip = True, vertical_flip = True)





def DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 4, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = False):

  from tensorflow.keras.applications.vgg16 import VGG16
  from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D
  from tensorflow.keras.optimizers import Adam
  from tensorflow.keras.models import Model, Sequential
  from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
  import matplotlib.pyplot as plt

  # Modèle VGG16
  base_model = VGG16(weights='imagenet', include_top=False)

  # Freeze toutes les couches du VGG16 sauf les n dernières (si n différent de 0)
  for layer in base_model.layers:
    layer.trainable = False

  if n_layers_trainable != 0:
    for layer in base_model.layers[- n_layers_trainable:]:
      layer.trainable = True

  # Callbacks
  early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0.01, mode ='min', patience = 6, verbose = 1)
  reduce_learning_rate = ReduceLROnPlateau (monitor = 'val_loss', factor = 0.1, patience = 3, min_delta = 0.01, cooldown = 4, verbose = 1)
  callbacks = [reduce_learning_rate, early_stopping]

  # Construction du modèle
  model = Sequential()
  model.add(base_model)
  model.add(GlobalAveragePooling2D())
  model.add(Dense(1024,activation='relu'))
  model.add(Dropout(rate=0.2))
  model.add(Dense(512, activation='relu'))
  model.add(Dropout(rate=0.2))
  model.add(Dense(n_class, activation='softmax'))

  model.compile(optimizer=Adam(learning_rate= learning_rate), loss='sparse_categorical_crossentropy', metrics=['acc'])

  # Entrainement du modèle

  print("Entrainement du modèle")
  history = model.fit(train_generator,
                      epochs = nb_epochs,
                      steps_per_epoch = train_generator.samples//batch_size,
                      validation_data = test_generator,
                      validation_steps = test_generator.samples//batch_size,
                      callbacks = callbacks)


  # Courbe de la fonction de coût et de précision en fonction de l'epoch

  print("\nCourbes de perte et de précision pour VGG16 avec les paramètres:\n # couches entrainées:", n_layers_trainable, "\n # learning rate init:", learning_rate,"\n # epochs            :",nb_epochs,"\n # batch size        :", batch_size)

  train_loss = history.history["loss"]
  val_loss = history.history["val_loss"]
  train_acc = history.history["acc"]
  val_acc = history.history["val_acc"]

  plt.figure(figsize = (20, 7))

  plt.subplot(121)
  plt.plot(train_loss)
  plt.plot(val_loss)
  plt.title('Model loss per epoch')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='right')
  plt.grid(True)

  plt.subplot(122)
  plt.plot(train_acc)
  plt.plot(val_acc)
  plt.title('Model accuracy per epoch')
  plt.ylabel('acc')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='right')
  plt.grid(True)
    
  plt.show();

  print("\n", model.summary())

    # Evaluation du modèle (un peu long donc désactivée par défaut)
  if model_eval == True:
    print("\nEvaluation du modèle sur l'ensemble de test augmenté par génération de données:")
    score = model.evaluate(test_generator)
    return model, history, score
  else:
    return model, history


model_4_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 4, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)
DidSave(model_4_64, '/content/drive/MyDrive/BD/model_4_layers_64_batch')


model_0_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 0, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)
DidSave(model_0_64, '/content/drive/MyDrive/BD/model_0_layers_64_batch')


model_12_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 12, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)


model_21_32,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 21, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = True)


model_21_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 21, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)





def DidSave(variable, fichier):
  import pickle
  f = open(fichier,"wb")
  pickle.dump(variable,f)
  f.close()

def DidLoad(fichier):
  import pickle
  f = open(fichier,"rb")
  variable = pickle.load(f)
  f.close()
  return variable


#DidSave(model_12_64, '/content/drive/MyDrive/BD/model_12_layers_64_batch')


#DidSave(model_21_64, '/content/drive/MyDrive/BD/model_16_layers_64_batch')





def DidFeatureExtractionClassification(X, Y, model, test_size = 0.2, layers_output = 2 , xgb = False):

  # extraction des features de la couche 2 et entrainement d'un modèle de classification
  from sklearn.model_selection import train_test_split
  from tensorflow.keras.models import Model
  from tensorflow.keras.applications.vgg16 import preprocess_input

  X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size)

  intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[layers_output].output)
  print('Train feature extraction')
  X_train_features = intermediate_layer_model.predict(preprocess_input(X_train))
  print('Test feature extraction')
  X_test_features = intermediate_layer_model.predict(preprocess_input(X_test))

  # decision tree
  from sklearn.tree import DecisionTreeClassifier
  clf = DecisionTreeClassifier()
  clf.fit(X_train_features, Y_train)
  print('score de classification avec Arbre de décision :', clf.score(X_test_features, Y_test))

  # SVM
  from sklearn.svm import SVC
  clf = SVC()
  clf.fit(X_train_features, Y_train)
  print('score de classification avec SVM               :', clf.score(X_test_features, Y_test))

  # Random Forest
  from sklearn import ensemble
  clf = ensemble.RandomForestClassifier(n_jobs=-1)
  clf.fit(X_train_features, Y_train)
  print('score de classification avec Random Forest     :', clf.score(X_test_features, Y_test))

  # XGBoost (un peu long donc désactivée par défaut)
  if xgb == True:

    import xgboost as xgb
    clf = xgb.XGBClassifier()

    from sklearn import preprocessing
    le = preprocessing.LabelEncoder()
    le.fit(Y_train)
    Y_train_enc = le.transform(Y_train)
    Y_test_enc = le.transform(Y_test)

    clf.fit(X_train_features, Y_train_enc)
    print('score de classification avec XGBoost           :', clf.score(X_test_features, Y_test_enc))

  return X_train_features, X_test_features


#X,Y = DidPreprocessing(path_DS, img_width = 100, img_height = 100, drop_duplicates = True)
#DidSave(X,'/content/drive/MyDrive/BD/variable_X')
#DidSave(Y,'/content/drive/MyDrive/BD/variable_Y')


X = DidLoad('/content/drive/MyDrive/BD/variable_X')
Y = DidLoad('/content/drive/MyDrive/BD/variable_Y')


for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_21_32, layers_output = i , xgb = True)


for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_12_64, layers_output = i , xgb = True)


for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_21_64, layers_output = i , xgb = True)


model_12_32,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 12, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = True)
DidSave(model_12_32, '/content/drive/MyDrive/BD/model_12_layers_32_batch')
for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_12_32, layers_output = i , xgb = True)






# en utilisant les ensembles d'entrainement et de test standardisés et réduits à 2 dimensions

# decision tree
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, Y_train)
print('score de classification avec Arbre de décision :', clf.score(X_test, Y_test))

# SVM
from sklearn.svm import SVC
clf = SVC()
clf.fit(X_train, Y_train)
print('score de classification avec SVM               :', clf.score(X_test, Y_test))

# Random Forest
from sklearn import ensemble
clf = ensemble.RandomForestClassifier(n_jobs=-1)
clf.fit(X_train, Y_train)
print('score de classification avec Random Forest     :', clf.score(X_test, Y_test))

# XGBoost
import xgboost as xgb
clf = xgb.XGBClassifier()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(Y_train)
Y_train_enc = le.transform(Y_train)
Y_test_enc = le.transform(Y_test)

clf.fit(X_train, Y_train_enc)
print('score de classification avec XGBoost           :', clf.score(X_test, Y_test_enc))





#création des datasets d'entrainement et de test à partir des fichiers jpg

import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt
import time

#import cv2
import seaborn as sns

import pandas as pd
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras as K

from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras import callbacks

from tensorflow.keras.applications.vgg16 import preprocess_input

train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                   shear_range = 0.2, # plage d'étirement
                                   zoom_range = 0.2,  # plage d'agrandissement
                                   rotation_range = 359, # plage de rotation en degré
                                   horizontal_flip = True,  #retournement horizontal aléatoire
                                   vertical_flip = True,  #retournement vertical aléatoire
                                   )

test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)

train_generator = train_datagen.flow_from_directory(directory=path_DS_train,
                                                           class_mode ="sparse",
                                                           target_size=(100,100),
                                                           batch_size=32)

test_generator = test_datagen.flow_from_directory(directory=path_DS_test,
                                                  class_mode ="sparse",
                                                  target_size=(100,100),
                                                  batch_size=32)



print("modele2 : C2 143 - 224x224 - lr 1e-4")

inputs = K.Input(shape=(224, 224, 3))

n_class = 9

resnet2 = K.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)

for layer in resnet2.layers[:143]:
    layer.trainable = False

model2 = K.models.Sequential()
model2.add(K.layers.Lambda(lambda x: tf.image.resize(x, (224, 224))))
model2.add(resnet2)

model2.add(Flatten())
model2.add(Dense(512, activation='relu'))
model2.add(Dense(10, activation='softmax'))

ces = callbacks.EarlyStopping(monitor='accuracy', patience=4, mode='max', restore_best_weights=True)
crop = callbacks.ReduceLROnPlateau(monitor='accuracy', patience=2, verbose=2, mode='max')

model2.compile(loss='sparse_categorical_crossentropy',
              optimizer=K.optimizers.legacy.RMSprop(1e-4),
              metrics=['accuracy'])

history2 = model2.fit(train_generator,
                    batch_size=32,
                    epochs=100,
                    verbose=1,
                    validation_data=test_generator,
                    shuffle=True,
                    callbacks=[ces, crop])


train_loss = history2.history["loss"]
val_loss = history2.history["val_loss"]
train_acc = history2.history["accuracy"]
val_acc = history2.history["val_accuracy"]

plt.figure(figsize = (20, 8))

plt.subplot(121)
plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Model loss per epoch - Modele2')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='right')


plt.subplot(122)
plt.plot(train_acc)
plt.plot(val_acc)
plt.title('Model accuracy per epoch - Modele2')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='right')

plt.show();


model2.summary()


#permet de charger un modèle et de sortir son évaluation
model2 = tf.keras.models.load_model('/content/drive/MyDrive/BD/vf_model2')
model2.evaluate(test_generator)


#on donne le chemin d'une image enregistrée sur pc
#et il ressort la classe prédite avec la probabilité
def test_from_local(chemin, model, test_generator):
    from tensorflow.keras.preprocessing import image
    import matplotlib.image as mpimg
    import numpy as np
    #met les images à la bonne dimension
    img = image.load_img(chemin, target_size=(100, 100))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    #obtenir l'indice de la classe la plus probable
    arg = int(tf.argmax(model.predict(x), axis=1).numpy())
    #obtenir la liste des noms des classes
    class_names = list(test_generator.class_indices.keys())
    #valeur de la probabilité que l'image appartienne à la classe
    proba = model.predict(x)[0][arg]

    return print("Cette image a", proba*100, "% de chance d'être un", class_names[arg])



#quelques tests de la fonction précdente (qui fonctionne bien)
test_from_local("/content/drive/MyDrive/BD/a_tester_eosinophil.jpg", model2, test_generator)


#on donne le chemin d'une image enregistrée sur pc
#et il ressort la classe prédite avec la probabilité
def test_from_url(image_url, model, test_generator):
    from tensorflow.keras.preprocessing import image
    import matplotlib.image as mpimg
    import numpy as np
    import urllib.request
    import io

    with urllib.request.urlopen(image_url) as url:
        image_data = url.read()

    img = image.load_img(io.BytesIO(image_data), target_size=(100, 100))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    arg = int(tf.argmax(model.predict(x), axis=1).numpy())

    class_names = list(test_generator.class_indices.keys())
    proba = model.predict(x)[0][arg]

    return print("Cette image a", proba*100, "% de chance d'être un", class_names[arg])



#quelques tests de la fonction précdente (qui fonctionne bien)
test_from_url('http://bioimage.free.fr/hem_image/hem_img/pb32l.jpg', model2, test_generator)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input , Dense
from tensorflow.keras.models import Model

import tensorflow as tf


#Rechargement des données sauvegardées

X_dnet121 = X_reload
Y_dnet121 = Y_reload


Y_dnet121





#Fonction permettant de préparer les données pour petre conforme à l'entrée du DenseNet121

from sklearn.preprocessing import LabelEncoder
import tensorflow as tf

encoder = LabelEncoder()
encoder.fit(Y_dnet121)
saved_labels = encoder.inverse_transform([i for i in range(9)])

def preprocess_data(X, Y):

    X = np.float32(X)
    Y = np.array(Y)
    """
    function that pre-processes th dataset as per
    densenet model requirements for input images
    labels are one-hot encoded
    """
    encoder = LabelEncoder()
    X = tf.keras.applications.densenet.preprocess_input(X)
    Y = encoder.fit_transform(Y)
    Y = tf.keras.utils.to_categorical(Y , 9)
    return X, Y



from sklearn.model_selection import train_test_split

X_dnet121_train_base , X_dnet121_test_base , Y_dnet121_train_base , Y_dnet121_test_base = train_test_split( X_dnet121 , Y_dnet121 , test_size = 0.2 )



#Preprocessing train data pour le densenet121

X_dnet121_train , Y_dnet121_train = preprocess_data( X_dnet121_train_base , Y_dnet121_train_base )


#Preprocessing test data pour le densenet121

X_dnet121_test , Y_dnet121_test = preprocess_data( X_dnet121_test_base , Y_dnet121_test_base )


#Implémentation du DenseNet121 et gel des 150 premières couches

base_densenet121 = tf.keras.applications.DenseNet121( include_top = False , weights = 'imagenet' )


#Gel des 150 premières couches

for layer in base_densenet121.layers[ :149 ] :
  layer.trainable = False
for layer in base_densenet121.layers[ 149: ] :
  layer.trainable = True




#Consutrction du modèle

model_densenet121 = tf.keras.models.Sequential()


#Mise en forme des données pour le DenseNet121

model_densenet121.add( tf.keras.layers.Lambda( lambda x : tf.keras.backend.resize_images( x ,
                                                              height_factor = 1 ,
                                                              width_factor = 1 ,
                                                              data_format = 'channels_last' )  ) )



#Construction du modèle (classifieur)

kernel_init = 'normal'

model_densenet121.add( base_densenet121 )
model_densenet121.add( tf.keras.layers.Flatten() )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(512, activation='relu',kernel_initializer=kernel_init) )
model_densenet121.add( tf.keras.layers.Dropout(0.7) )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(128, activation='relu',kernel_initializer=kernel_init))
model_densenet121.add( tf.keras.layers.Dropout(0.5) )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(64, activation='relu',kernel_initializer=kernel_init) )
model_densenet121.add( tf.keras.layers.Dropout(0.3) )
model_densenet121.add( tf.keras.layers.Dense(9, activation='softmax',kernel_initializer=kernel_init) )



#Callbacks

from tensorflow.keras import callbacks

CB = []

CB.append(callbacks.ModelCheckpoint( filepath='CB_best' , monitor='val_accuracy', save_best_only=True ))

CB.append( callbacks.EarlyStopping( monitor='val_accuracy', min_delta = 0.01 , patience = 5 ) )

CB.append( callbacks.ReduceLROnPlateau (monitor = 'val_loss', factor = 0.1, patience = 3, min_delta = 0.01, cooldown = 4, verbose = 1) )



#Compile

optimizer = 'Adam'

model_densenet121.compile( optimizer = optimizer, loss= 'categorical_crossentropy' , metrics=['accuracy'] )


#Train

history_densenet121 = model_densenet121.fit( x = X_dnet121_train , y = Y_dnet121_train ,
                                 batch_size = 32 , epochs = 30 ,
                                 callbacks = CB , validation_data = ( X_dnet121_test , Y_dnet121_test ) ,
                                  verbose = True )



#Enregistrement du modèle

import pickle

pickle_out = open('/content/drive/MyDrive/BD/model_densenet121.pckl', 'wb')
pickle.dump(model_densenet121, pickle_out)
pickle_out.close()



#Enregistrement de l'historique

pickle_out = open('/content/drive/MyDrive/BD/history_densenet121.pckl', 'wb')
pickle.dump(history_densenet121, pickle_out)
pickle_out.close()


model_densenet121.summary()


#Affichage de l'évolution de l'accuracy de la loss

train_acc = history_densenet121.history['accuracy']

val_acc = history_densenet121.history['val_accuracy']


plt.plot(train_acc , label = "train accuracy")
plt.plot(val_acc , label = "test accuracy")
plt.title('DenseNet121 : Accuracy')
plt.legend( loc = 'lower right' )
plt.grid(True)
plt.show();


train_loss = history_densenet121.history['loss']

val_loss = history_densenet121.history['val_loss']


plt.plot(train_loss , label = "train loss")
plt.plot(val_loss , label = "test loss")
plt.title('DenseNet121 : Loss')
plt.legend( loc = 'upper right' )
plt.grid(True)
plt.show();


#Prédictions du modèle

probs_pred_densenet121 = model_densenet121.predict( X_dnet121_test )



Y_pred_densenet121 = np.argmax( probs_pred_densenet121 , axis = 1 )


Y_test_densenet121_sparse = np.argmax( Y_dnet121_test , axis = 1 )


from sklearn.metrics import confusion_matrix

conf_matrix_densenet121 = confusion_matrix( Y_test_densenet121_sparse , Y_pred_densenet121 )

print(conf_matrix_densenet121)


from sklearn.metrics import classification_report

class_report_densenet121 = classification_report( Y_test_densenet121_sparse , Y_pred_densenet121 )

print( class_report_densenet121 )


#Implémentation du DenseNet121 sans geler aucun calque

base_densenet121_nf = tf.keras.applications.DenseNet121( include_top = False , weights = 'imagenet' )


model_densenet121_nf = tf.keras.models.Sequential()


model_densenet121_nf.add( tf.keras.layers.Lambda( lambda x : tf.keras.backend.resize_images( x ,
                                                              height_factor = 1 ,
                                                              width_factor = 1 ,
                                                              data_format = 'channels_last' )  ) )


#Construction du modèle (même classifieur)

kernel_init = 'normal'

model_densenet121_nf.add( base_densenet121_nf )
model_densenet121_nf.add( tf.keras.layers.Flatten() )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(512, activation='relu',kernel_initializer=kernel_init) )
model_densenet121_nf.add( tf.keras.layers.Dropout(0.7) )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(128, activation='relu',kernel_initializer=kernel_init))
model_densenet121_nf.add( tf.keras.layers.Dropout(0.5) )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(64, activation='relu',kernel_initializer=kernel_init) )
model_densenet121_nf.add( tf.keras.layers.Dropout(0.3) )
model_densenet121_nf.add( tf.keras.layers.Dense(9, activation='softmax',kernel_initializer=kernel_init) )



#Compile

optimizer = 'Adam'

model_densenet121_nf.compile( optimizer = optimizer, loss= 'categorical_crossentropy' , metrics=['accuracy'] )


#Train

history_densenet121_nf = model_densenet121_nf.fit( x = X_dnet121_train , y = Y_dnet121_train ,
                                 batch_size = 32 , epochs = 30 ,
                                 callbacks = CB , validation_data = ( X_dnet121_test , Y_dnet121_test ) ,
                                  verbose = True )



#Enregistrement du modèle et de l'historique

import pickle

pickle_out = open('/content/drive/MyDrive/BD/model_densenet121_nf.pckl', 'wb')
pickle.dump(model_densenet121_nf, pickle_out)
pickle_out.close()





pickle_in = open('/content/drive/MyDrive/BD/model_densenet121_nf.pckl' , 'rb')
model_densenet121_nf = pickle.load(pickle_in)


pickle_out = open('/content/drive/MyDrive/BD/history_densenet121_nf.pckl', 'wb')
pickle.dump(history_densenet121_nf, pickle_out)
pickle_out.close()


model_densenet121_nf.summary()


#Courbes de l'accurarcy et de la loss

train_acc_nf = history_densenet121_nf.history['accuracy']

val_acc_nf = history_densenet121_nf.history['val_accuracy']


plt.plot(train_acc_nf , label = "train accuracy")
plt.plot(val_acc_nf , label = "test accuracy")
plt.title('DenseNet121 : Accuracy')
plt.legend( loc = 'lower right' )
plt.grid(True)
plt.show();


train_loss_nf = history_densenet121_nf.history['loss']

val_loss_nf = history_densenet121_nf.history['val_loss']


plt.plot(train_loss_nf , label = "train loss")
plt.plot(val_loss_nf , label = "test loss")
plt.title('DenseNet121 : Loss')
plt.legend( loc = 'upper right' )
plt.grid(True)
plt.show();


#Prédictions du modèle

probs_pred_densenet121_nf = model_densenet121_nf.predict( X_dnet121_test )



Y_pred_densenet121_nf = np.argmax( probs_pred_densenet121_nf , axis = 1 )


Y_test_densenet121_sparse = np.argmax( Y_dnet121_test , axis = 1 )


#Matrice de confusion et rapports de classification

from sklearn.metrics import confusion_matrix

conf_matrix_densenet121_nf = confusion_matrix( Y_test_densenet121_sparse , Y_pred_densenet121_nf )

print(conf_matrix_densenet121_nf)


from sklearn.metrics import classification_report

class_report_densenet121_nf = classification_report( Y_test_densenet121_sparse , Y_pred_densenet121_nf )

print( class_report_densenet121_nf )


from imblearn.metrics import classification_report_imbalanced

print(classification_report_imbalanced( Y_test_densenet121_sparse , Y_pred_densenet121_nf ))


#Génération et classification d'images du test set

import random

plt.figure( figsize = ( 10 , 6 ) )

for i in range(8) :

  k = random.randint( 0 , len(X_dnet121_test_base) )
  plt.subplot( 2 , 4 , i+1 )
  plt.imshow( X_dnet121_test_base[k] )
  plt.axis(False)
  plt.title( saved_labels[Y_test_densenet121_sparse[k]]
              + '\npredicted as\n'
              + saved_labels[Y_pred_densenet121_nf[k]] )
plt.show();






