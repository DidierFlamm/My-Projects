# -*- coding: utf-8 -*-
"""Blood_Cells_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0t5SUU8_sodD_RKLLwt5u_zESxrWJnt

# Introduction

Ce projet s'inscrit dans le cadre de la formation en Data Science dispensée par DataScientest.com.

L’objectif de ce projet est d’identifier les différents types de cellules du sang à l’aide d'algorithmes de computer vision.

Paramétrage des chemins d'accès aux images sur Google Drive
"""

from google.colab import drive
drive.mount('/content/drive/')

# Stockage des DS initiaux
path_DS1 = '/content/drive/MyDrive/BD/PBC_dataset_normal_DIB/'
path_DS2 = '/content/drive/MyDrive/BD/Kaggle_APL'
path_DS3 = '/content/drive/MyDrive/BD/Kaggle_LD'

# Stockage du DS final = Data Set 1 complet + images _0 du dossier IDB2 du Data Set 2

path_DS = '/content/drive/MyDrive/BD/CELLULES_SAINES'

# Stockage du DS complet et des ensembles splités (train à 80% et test à 20%) au format RGB 100x100 sans doublon
path_DS_processed = '/content/drive/MyDrive/BD/IDB100x100'
path_DS_train = '/content/drive/MyDrive/BD/TRAIN_RGB'
path_DS_test = '/content/drive/MyDrive/BD/TEST_RGB'

# Enregistrement des images après segmentation
path_DS_train_seg = '/content/drive/MyDrive/BD/TRAIN_SEG'
path_DS_test_seg = '/content/drive/MyDrive/BD/TEST_SEG'

"""# I. Data Visualisation

Définition d'une fonction qui compte le nombre d'images dans chaque sous-dossier d'un répertoire et affiche le résultat
"""

# Commented out IPython magic to ensure Python compatibility.
def DidDataViz(directory):

  import os
  compteur = {}
  for (root, dirs , files) in os.walk(directory):
    cat = root.split("/")[-1]
    if len(files)>0:
      if cat not in compteur.keys():
          compteur[cat] = 0
      compteur[cat] += len(files)



  print('Found', sum(compteur.values()), 'images belonging to', len(compteur.keys()), 'classes')

  # Data Viz'
  import matplotlib.pyplot as plt
#   %matplotlib inline
  plt.figure(figsize=(12, 5))

  # graph en barre par ordre croissant

  compteur_inc = dict(sorted(compteur.items(), key=lambda x: x[1])) #tri par valeurs croissantes

  plt.subplot(121)
  plt.bar(x=compteur_inc.keys(), height= compteur_inc.values())
  plt.xticks(rotation=70);

  # camembert par ordre alphabétique

  compteur = dict(sorted(compteur.items(), key=lambda x: x[0])) #tri alphabétique des clés

  lq = []
  for elem in compteur.items():
      lq.append(elem[0] + "\n(" + str(elem[1]) + ")") #label avec nom et quantité

  plt.subplot(122)
  plt.pie(x = compteur.values(),
          labels = lq,
          autopct = lambda x: str(round(x, 1)) + ' %',
          pctdistance = 0.7,
          labeldistance = 1.2);

  plt.show();

  return compteur

"""##1. Data Set 1"""

DidDataViz(path_DS1)

"""##2. Data Set 2"""

DidDataViz(path_DS2)

"""##3. Data Set 3"""

DidDataViz(path_DS3)

"""##4. Data Set Complet"""

DidDataViz(path_DS)

"""# II. Pre-Processing

##1. Redimensionnement et suppression des doublons
"""

def DidPreprocessing(directory, img_width = 100, img_height = 100, drop_duplicates = False):

  import cv2, os, gc
  import numpy as np
  import pandas as pd

  # définition de la liste des sous-dossiers du répertoire

  subdir=[]
  for file in os.listdir(directory):
      d = os.path.join(directory, file)
      if os.path.isdir(d):
          subdir.append(file)

  # Charger le set d'image dans X et Y

  img_list = []
  label_list = []
  name_list = []

  for cat in subdir:
      path = os.path.join(directory,cat)+'/'
      for filename in os.listdir(path):
          img=cv2.imread(os.path.join(path,filename))
          # Resize image
          img=cv2.resize(img,(img_width, img_height))
          # for the black and white image
          if img.shape==(img_width, img_height):
              img=img.reshape([img_width,img_height,1])
              img=np.concatenate([img,img,img],axis=2)
          # cv2 load the image BGR sequence color (not RGB)
          img_list.append(img[...,::-1])
          label_list.append(cat)
          name_list.append(filename)

  X, Y = np.array(img_list), np.array(label_list)
  labels = np.unique(Y)
  print('Loaded', len(Y), 'images belonging to', len(labels), 'classes')

  # Supprimer doublons
  if drop_duplicates == True:
    import matplotlib.pyplot as plt
    print('Vérification des doublons...')
    doublons = []
    for i in range(len(X)-1,0,-1):
      for j in range(0,i):
        if (X[i]==X[j]).all():
          doublons.append([j,i])

          print(str(len(doublons))+'.','suppression de', label_list[i], name_list[i],"en doublon avec", label_list[j], name_list[j])

          plt.subplot(121)
          plt.imshow(X[i])
          plt.title(Y[i] + "\n" + name_list[i])
          plt.axis(False)

          plt.subplot(122)
          plt.imshow(X[j])
          plt.title(Y[j] + "\n" + name_list[j])
          plt.axis(False)

          plt.show();

          X = np.delete(X,i,0)
          Y = np.delete(Y,i,0)

          break
    print('Found and deleted', len(doublons), 'duplicates')
    labels = np.unique(Y)
    print(len(Y), 'images left after removing duplicates, belonging to', len(labels), 'classes:', labels)

  return X,Y

X,Y = DidPreprocessing(path_DS, img_width = 100, img_height = 100, drop_duplicates = True)

#Enregistrement de données importées sans traitement

import pickle

pickle_out = open('/content/drive/MyDrive/BD/X_saved.pckl', 'wb')
pickle.dump(X, pickle_out)
pickle_out.close()

pickle_in = open('/content/drive/MyDrive/BD/X_saved.pckl' , 'rb')
X_reload = pickle.load(pickle_in)

pickle_out = open('/content/drive/MyDrive/BD/Y_saved.pckl', 'wb')
pickle.dump(Y, pickle_out)
pickle_out.close()

pickle_in = open('/content/drive/MyDrive/BD/Y_saved.pckl' , 'rb')
Y_reload = pickle.load(pickle_in)

"""##2. Stockage des images obtenues"""

def DidSaveImages(X,Y, dossier):
  import cv2, os
  import pandas as pd
  compteurs = {}
  # créer le dossier si il n'existe pas
  try:
    os.mkdir(dossier)
  except:
    print('impossible de créer le dossier', dossier)
  # créer tous les sous-dossiers
  for types in pd.DataFrame(Y)[0].unique():
    compteurs[types] = 1
    try:
      os.mkdir(dossier + '/' + types)
    except:
      print('impossible de créer le sous-dossier', types)

  for n, image in enumerate(X):
    filename = Y[n] + "_" + str(compteurs[Y[n]]) + '.jpg'
    compteurs[Y[n]] += 1
    if len(np.asarray(X_seg).shape)==4:
      img = cv2.cvtColor(X[n]*255, cv2.COLOR_BGR2RGB) # conversion BGR en RGB pour les images en couleur (et multiplie par 255)
    else:
      img = X[n]*255
    cv2.imwrite(dossier + '/' + Y[n] + '/' + filename, img)

# DidSaveImages(X,Y, dossier = path_DS_processed)

"""##3. Standarisation et Séparation en 2 ensembles de test et d'entrainement"""

import numpy as np
import pandas as pd

# données non standardisées
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)

#DidSaveImages(X_train,Y_train, dossier = path_DS_train)
#DidSaveImages(X_test,Y_test, dossier = path_DS_test)

# standardisation

X = np.float32(X)
Y = np.array(Y)
X /= 255

print('Vérification du Min: %.3f, et du Max: %.3f' % (X.min(), X.max()))

print(X_train.shape)
print(Y_train.shape)
print(Y_test.shape)

#reshape de X en un array à 2 dimensions
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)

#Division des données non segmentées en train et test

X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)

print(X_train.shape)
print(Y_train.shape)
print(Y_test.shape)

"""##4. Rééchantillonage par OverSampling et UnderSampling

### OverSampling
"""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
import pandas as pd
import numpy as np

# Réorganiser les dimensions des données d'images en une seule dimension
X2D = np.reshape(X, (len(X), -1))
print('shape de X2D:', X2D.shape)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X2D, Y, test_size=0.2)

# Sur-échantillonnage des données d'entraînement
over_sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
X_train_resampled, Y_train_resampled = over_sampler.fit_resample(X_train, Y_train)

print("Quantités pour X_train :", len(X_train))
print("Quantités pour X_test :", len(X_test))
print("Quantités pour Y_train :", len(Y_train))
print("Quantités pour Y_test :", len(Y_test))
print("Quantités pour X_train_resampled :", len(X_train_resampled))
print("Quantités pour Y_train_resampled :", len(Y_train_resampled))

# Labels pour le diagramme circulaire
labels = ['basophil', 'lymphoblast', 'erythroblast', 'platelet', 'lymphocyte', 'eosinophil', 'ig', 'neutrophil', 'monocyte']

# Quantités pour chaque classe oversampler
oversampled_quantities = np.unique(Y_train_resampled, return_counts=True)[1]

# Afficher les quantités de chaque classe oversampler
for label, quantity in zip(labels, oversampled_quantities):
    print(f"Classe {label}: {quantity} données")

#VIZ

import matplotlib.pyplot as plt

# Compter les occurrences de chaque classe dans les ensembles d'entraînement
_, train_counts = np.unique(Y_train, return_counts=True)
_, resampled_counts = np.unique(Y_train_resampled, return_counts=True)

# Labels pour le diagramme circulaire
labels = ['basophil', 'lymphoblast', 'erythroblast', 'platelet', 'lymphocyte', 'eosinophil', 'ig', 'neutrophil', 'monocyte']

# Données pour le diagramme circulaire
train_data = train_counts / len(Y_train) * 100  # Convertir en pourcentage
resampled_data = resampled_counts / len(Y_train_resampled) * 100  # Convertir en pourcentage

# Créer la figure et les sous-graphiques
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Diagramme circulaire pour X_train et Y_train
ax1.pie(train_data, labels=labels, autopct='%1.1f%%')
ax1.set_title('X_train, Y_train')

# Diagramme circulaire pour X_train_resampled et Y_train_resampled
ax2.pie(resampled_data, labels=labels, autopct='%1.1f%%')
ax2.set_title('X_train_resampled, Y_train_resampled')

# Ajuster l'espacement entre les sous-graphiques
plt.subplots_adjust(wspace=0.4)

# Afficher le diagramme circulaire
plt.show()

"""###  UnderSampling"""

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
import pandas as pd
import numpy as np

# Réorganiser les dimensions des données d'images en deux dimensions
X2D = np.reshape(X, (len(X), -1))
print('shape de X2D:', X2D.shape)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X2D, Y, test_size=0.2)

# Sous-échantillonnage des données d'entraînement
under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)
X_train_resampled, Y_train_resampled = under_sampler.fit_resample(X_train, Y_train)

print("Quantités pour X_train :", len(X_train))
print("Quantités pour X_test :", len(X_test))
print("Quantités pour Y_train :", len(Y_train))
print("Quantités pour Y_test :", len(Y_test))
print("Quantités pour X_train_resampled :", len(X_train_resampled))
print("Quantités pour Y_train_resampled :", len(Y_train_resampled))

# Labels pour le diagramme circulaire
labels = ['basophil', 'lymphoblast', 'erythroblast', 'platelet', 'lymphocyte', 'eosinophil', 'ig', 'neutrophil', 'monocyte']

# Quantités pour chaque classe undersampler
undersampled_quantities = np.unique(Y_train_resampled, return_counts=True)

# Afficher les quantités de chaque classe undersampler
for i in range(len(labels)):
    print(f"Classe {labels[i]}: {undersampled_quantities[1][i]} données")

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler


# Réorganiser les dimensions des données d'images en deux dimensions
X2D = np.reshape(X, (len(X), -1))
print('shape de X2D:', X2D.shape)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X2D, Y, test_size=0.2)


# Créer l'objet RandomUnderSampler
under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)

# Undersampling des données d'entraînement
X_train_undersampled, Y_train_undersampled = under_sampler.fit_resample(X_train, Y_train)

# Compter les occurrences de chaque classe dans les ensembles d'entraînement et d'undersampling
_, train_counts = np.unique(Y_train, return_counts=True)
_, undersampled_counts = np.unique(Y_train_undersampled, return_counts=True)

# Labels pour le diagramme circulaire
labels = ['basophil', 'lymphoblast', 'erythroblast', 'platelet', 'lymphocyte', 'eosinophil', 'ig', 'neutrophil', 'monocyte']

# Données pour le diagramme circulaire des vraies données
train_data = train_counts / len(Y_train) * 100  # Convertir en pourcentage

# Données pour le diagramme circulaire des données undersampled
undersampled_data = undersampled_counts / len(Y_train_undersampled) * 100  # Convertir en pourcentage

# Créer la figure et les sous-graphiques
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Diagramme circulaire pour les vraies données
ax1.pie(train_data, labels=labels, autopct='%1.1f%%')
ax1.set_title('Vraies données')

# Diagramme circulaire pour les données undersampled
ax2.pie(undersampled_data, labels=labels, autopct='%1.1f%%')
ax2.set_title('Données undersampled')

# Ajuster l'espacement entre les sous-graphiques
plt.subplots_adjust(wspace=0.4)

# Afficher les diagrammes circulaires
plt.show()

"""##5. Segmentation des 2 ensembles"""

#mise de côté du jeu original pour faire les tests
x_jo = X
y_jo = Y

#nous allons faire quelques tests ici sur Segmentation by Thresholding
#le but est de trouver la valeur idéale du seuil
from skimage import data
from skimage.color import rgb2gray
import matplotlib.pyplot as plt
from skimage import filters

# Setting the plot size to 15,15
plt.figure(figsize=(10, 10))

# On prend une image du jeu de donnée
img_hasard = x_jo[23]
plt.subplot(1, 2, 1)

# Displaying the sample image
plt.imshow(img_hasard)

# Converting RGB image to Monochrome
gray_img_hasard = rgb2gray(img_hasard)
plt.subplot(1, 2, 2)

# Displaying the sample image - Monochrome
# Format
plt.imshow(gray_img_hasard, cmap="gray")

# Sample Image of scikit-image package
gray_img_hasard = rgb2gray(img_hasard)

# Setting the plot size to 15,15
plt.figure(figsize=(10, 10))

for i in range(10):

  # Iterating different thresholds
  binarized_gray = (gray_img_hasard > i*0.1)*1
  plt.subplot(5,2,i+1)

  # Rounding of the threshold
  # value to 1 decimal point
  plt.title("Threshold: >"+str(round(i*0.1,1)))

  # Displaying the binarized image
  # of various thresholds
  plt.imshow(binarized_gray, cmap = 'gray')

plt.tight_layout()

#affichage de l'image segmentée avec le seuil (0.5) choisi
image_gray = (gray_img_hasard > 0.5)*1
plt.imshow(image_gray, cmap = 'gray')

#affichage sur une image au hasard afin de voir ce que ça donne
#comparaison entre image de départ /
#image en niveaux de gris / image segmentée
from random import randint

alea=randint(0, len(x_jo))

plt.figure(figsize=(15, 15))

# Sample Image of scikit-image package
image_ch = x_jo[alea]
plt.subplot(1, 3, 1)

# Displaying the sample image
plt.imshow(image_ch);


# Converting RGB image to Monochrome
gray_image_ch = rgb2gray(image_ch)
plt.subplot(1, 3, 2)

# Displaying the sample image - Monochrome
# Format
plt.imshow(gray_image_ch, cmap="gray");

image_ch_gray2 = (gray_image_ch > 0.5)*1
plt.subplot(1, 3, 3)
plt.imshow(image_ch_gray2, cmap = 'gray');

#affichage de 100 images afin de vérifier si le seuil est bien choisi
#pour un plus grand nombre d'images
for i in range(100):
  plt.figure(figsize=(5, 10))

  image_ch = x_jo[i]
  plt.subplot(1, 3, 1)

  plt.imshow(image_ch);

  gray_image_ch = rgb2gray(image_ch)
  plt.subplot(1, 3, 2)

  plt.imshow(gray_image_ch, cmap="gray");

  image_ch_gray2 = (gray_image_ch > 0.5)*1

  plt.subplot(1, 3, 3)

  plt.imshow(image_ch_gray2, cmap = 'gray');

#création d'une liste test pour appliquer le masque
x_jo_test = []
for i in range(10):
  image_ch = x_jo[i]
  gray_image_ch = rgb2gray(image_ch)
  image_ch_gray2 = (gray_image_ch > 0.5)*1
  x_jo_test.append(image_ch_gray2)

x_jo_test = np.array(x_jo_test)
x_jo_test.shape

#affichage des résultats
for i in range(len(x_jo_test)):
  plt.figure(figsize=(2,2))
  plt.imshow(x_jo_test[i], cmap = 'gray');

#application à l'ensemble des images désormais
x_jo_seg = []
for i in range(len(x_jo)):
  image_ch = x_jo[i]
  gray_image_ch = rgb2gray(image_ch)
  image_ch_gray2 = (gray_image_ch > 0.5)*1
  x_jo_seg.append(image_ch_gray2)

x_jo_seg = np.array(x_jo_seg)
x_jo_seg.shape

#visualisation d'images au hasard de la liste complète
#comparaison entre image de dpéart et image segmentée
import random
for i in range(10):
  r = random.randint(0, len(x_jo_seg))
  plt.figure(figsize=(5,10))
  plt.subplot(1, 2, 1)
  plt.imshow(x_jo[r])

  plt.subplot(1, 2, 2)
  plt.imshow(x_jo_seg[r], cmap = 'gray');

#dataset segmenté
#X_seg = np.array(x_jo_seg)
#Y_seg = np.array(Y)

#print(X_seg.shape)
#print(Y_seg.shape)

#code plus court qui résume toutes les étapes
#et génére les datasets segmentés
from skimage.color import rgb2gray
X_seg = []
for image in X:
  image_gray = rgb2gray(image)
  image_seg = (image_gray > 0.5)*1
  X_seg.append(image_seg)

np.asarray(X_seg).shape

X_seg_train, X_seg_test, Y_train, Y_test = train_test_split(X_seg,Y,test_size=0.2)

#fonction pour extraire les fichiers dans le dossier
def images_arbo(X,Y, dossier):
  import cv2, os
  import pandas as pd
  compteurs = {}
  # créer le dossier si il n'existe pas
  try:
    os.mkdir(dossier)
  except:
    print('impossible de créer le dossier', dossier)
  # créer tous les sous-dossiers
  for types in pd.DataFrame(Y)[0].unique():
    compteurs[types] = 1
    try:
      os.mkdir(dossier + '/' + types)
    except:
      print('impossible de créer le sous-dossier', types)

  for n, image in enumerate(X):
    filename = Y[n] + "_" + str(compteurs[Y[n]]) + '.jpg'
    compteurs[Y[n]] += 1
    if len(np.asarray(X).shape)==4:
      img = cv2.cvtColor(X[n]*255, cv2.COLOR_BGR2RGB) # conversion BGR en RGB pour les images en couleur (et multiplie par 255)
    else:
      img = X[n]*255
    cv2.imwrite(dossier + '/' + Y[n] + '/' + filename, img)

#création des dossiers TRAIN et TEST SEG pour la Data Augmentation
images_arbo(X_seg_train,Y_train, path_DS_train_seg)
images_arbo(X_seg_test,Y_test, path_DS_test_seg)

"""# III. Machine Learning

##1. Random Forest

### Données non segmentées
"""

# Importation des bibliothèques nécessaires
from sklearn import ensemble
from sklearn.model_selection import train_test_split

# Création d'une instance de RandomForestClassifier,
# n_jobs=-1 signifie que le calcul doit être effectué en parallèle sur tous les cœurs de l'ordinateur
clf = ensemble.RandomForestClassifier(n_jobs=-1)
# Entraînement du modèle RandomForestClassifier sur le jeu de données d'entraînement
clf.fit(X_train, Y_train)

Y_pred_r = clf.predict(X_test)# Prédiction des étiquettes pour le jeu de données de test

pd.crosstab(Y_test, Y_pred_r) # Affichage de la matrice de confusion entre les étiquettes réelles du jeu de données de test et les étiquettes prédites

from  sklearn.metrics import classification_report
# Affichage du rapport de classification
print(classification_report(Y_test, Y_pred_r))

"""### Oversampling

Random Forest & Oversampling

On veut comparer les performances d'un modèle (RandomForestClassifier) utilisant les vraies données et d'un modèle (RandomForestClassifier) utilisant les données oversampler, en calculant la précision et en générant des rapports de classification distincts pour chaque modèle.
"""

from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier

# Entraînement et évaluation du modèle sur les vraies données
model_base = RandomForestClassifier()  # Modèle de forêt aléatoire pour les vraies données
model_base.fit(X_train, Y_train)
Y_pred_base = model_base.predict(X_test)

accuracy_base = accuracy_score(Y_test, Y_pred_base)
report_base = classification_report(Y_test, Y_pred_base)

# Entraînement et évaluation du modèle sur les données oversampler
model_oversampled = RandomForestClassifier()  # Modèle de forêt aléatoire pour les données oversampler
model_oversampled.fit(X_train_resampled, Y_train_resampled)
Y_pred_oversampled = model_oversampled.predict(X_test)

accuracy_oversampled = accuracy_score(Y_test, Y_pred_oversampled)
report_oversampled = classification_report(Y_test, Y_pred_oversampled)

# Affichage des scores et rapports de classification
print("Score - Base :", accuracy_base)

print("Score - Oversampled :", accuracy_oversampled)

print("\nRapport de classification - Base :\n", report_base)

print("\nRapport de classification - Oversampled :\n", report_oversampled)

"""### Undersampling"""

from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier

# Entraînement et évaluation du modèle sur les vraies données
model_base = RandomForestClassifier()  # Modèle de forêt aléatoire pour les vraies données
model_base.fit(X_train, Y_train)
Y_pred_base = model_base.predict(X_test)

accuracy_base = accuracy_score(Y_test, Y_pred_base)
report_base = classification_report(Y_test, Y_pred_base)

# Entraînement et évaluation du modèle sur les données undersampler
model_undersampled = RandomForestClassifier()  # Modèle de forêt aléatoire pour les données undersampler
model_undersampled.fit(X_train_undersampled, Y_train_undersampled)
Y_pred_undersampled = model_undersampled.predict(X_test)

accuracy_undersampled = accuracy_score(Y_test, Y_pred_undersampled)
report_undersampled = classification_report(Y_test, Y_pred_undersampled)

# Affichage des scores et rapports de classification
print("Score - Base :", accuracy_base)
print("Score - Undersampled :", accuracy_undersampled)

print("\nRapport de classification - Base :\n", report_base)
print("\nRapport de classification - Undersampled :\n", report_undersampled)

"""### Données segmentées"""

# Conversion de la liste X_seg_train en un tableau numpy pour une manipulation plus facile
X_seg_train = np.asarray(X_seg_train)

# Affichage de la forme du tableau X_seg_train pour vérifier sa dimension
print(X_seg_train.shape)

# Conversion de la liste X_seg_test en un tableau numpy pour une manipulation plus facile
X_seg_test = np.asarray(X_seg_test)

# Affichage de la forme du tableau X_seg_test pour vérifier sa dimension
print(X_seg_test.shape)

# Affichage de la forme de Y pour vérifier sa dimension. Y est le vecteur des étiquettes de classe.
print(Y.shape)

# Aplatissement des images segmentées
X_seg_train_2D = np.reshape(X_seg_train, (len(X_seg_train), -1))
X_seg_test_2D = np.reshape(X_seg_test, (len(X_seg_test), -1))

print(X_seg_train_2D .shape)
print(X_seg_test_2D .shape)

print(X_seg_train.shape)
print(X_seg_test.shape)
print(Y.shape)

# Importation des packages nécessaires
from sklearn import ensemble
from sklearn.model_selection import train_test_split

# Création d'un objet de la classe RandomForestClassifier

clf = ensemble.RandomForestClassifier(n_jobs=-1)

# Entraînement du modèle de forêt aléatoire en utilisant les données d'entraînement
# X_seg_train_2D contient les caractéristiques
# Y_train contient les étiquettes de classe correspondantes
clf.fit(X_seg_train_2D, Y_train)

# Le modèle entraîné est utilisé pour prédire les étiquettes de classe pour les données de test
Y_pred = clf.predict(X_seg_test_2D)

# pd.crosstab génère une matrice de confusion qui montre le nombre de prédictions correctes et incorrectes, segmentées par classe
pd.crosstab(Y_test, Y_pred)

# Importation de la fonction classification_report pour obtenir des statistiques détaillées sur la performance du modèle
from  sklearn.metrics import classification_report

# Affiche un rapport de classification, qui inclut précision, rappel, score F1 et support pour chaque classe
print(classification_report(Y_test, Y_pred))

# Score du modèle sur les données de test
# clf.score() retourne la précision
# En d'autres termes, la fraction des échantillons de test correctement classés.
score = clf.score(X_seg_test_2D, Y_test)
print(score)

"""##2. SVM

Données non segmentées
"""

from sklearn.svm import SVC
# Initialisation du classificateur SVC avec un noyau linéaire et une fonction de décision 'one vs one'
cls = SVC(kernel='linear', decision_function_shape='ovo')
# Entraînement du modèle SVM sur les données d'entraînement
cls.fit(X_train, Y_train)

# Utilisation du modèle entraîné pour prédire les étiquettes de classe pour les données de test
Y_pred = cls.predict(X_test)

# Génération d'une matrice de confusion pour montrer le nombre de prédictions correctes et incorrectes, classées par classe
pd.crosstab(Y_test, Y_pred)

# Importation de la fonction classification_report pour obtenir des statistiques détaillées sur la performance du modèle
from  sklearn.metrics import classification_report

print(classification_report(Y_test, Y_pred))

score = cls.score(X_test, Y_test)
print(score)

"""Données segmentées"""

# Importation du module SVM (Support Vector Machine) de sklearn
from sklearn.svm import SVC

# Initialisation du classificateur SVC avec un noyau linéaire et une fonction de décision 'one vs one'
cls = SVC(kernel='linear', decision_function_shape='ovo')

# Entraînement du modèle SVM sur les données d'entraînement
cls.fit(X_seg_train_2D, Y_train)

# Utilisation du modèle entraîné pour prédire les étiquettes de classe pour les données de test
Y_pred = cls.predict(X_seg_test_2D)

# Génération d'une matrice de confusion pour montrer le nombre de prédictions correctes et incorrectes, classées par classe
pd.crosstab(Y_test, Y_pred)

#
print(classification_report(Y_test, Y_pred))

# Calcul et affichage du score du modèle sur les données de test
score = cls.score(X_seg_test_2D, Y_test)
print(score)

"""##3. XGBoost

Données non segmentées
"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Réorganiser les dimensions des données d'images en une seule dimension
X2D = np.reshape(X, (len(X), -1))
print('shape de X2D:', X2D.shape)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X2D, Y, test_size=0.2)

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_train_encoded = label_encoder.fit_transform(Y_train)

# Création de l'estimateur XGBoost
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

# Entraînement du modèle
model.fit(X_train, Y_train_encoded)

# Prédiction sur les données de test
Y_pred_encoded = model.predict(X_test)

# Convertir les prédictions en format original
Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

# Affichage de la matrice de confusion
pd.crosstab(Y_test, Y_pred_xg)

# Affichage du rapport de classification
print(classification_report(Y_test, Y_pred_xg))

"""Données segmentées"""

X_seg_train, X_seg_test, Y_train, Y_test = train_test_split(X_seg,Y,test_size=0.2)
# Conversion de la liste X_seg_train en un tableau numpy pour une manipulation plus facile
X_seg_train = np.asarray(X_seg_train)

# Affichage de la forme du tableau X_seg_train pour vérifier sa dimension
print(X_seg_train.shape)

# Conversion de la liste X_seg_test en un tableau numpy pour une manipulation plus facile
X_seg_test = np.asarray(X_seg_test)

# Affichage de la forme du tableau X_seg_test pour vérifier sa dimension
print(X_seg_test.shape)

# Affichage de la forme de Y pour vérifier sa dimension. Y est le vecteur des étiquettes de classe.
print(Y.shape)

# Aplatissement des images segmentées
X_seg_train_2D = np.reshape(X_seg_train, (len(X_seg_train), -1))
X_seg_test_2D = np.reshape(X_seg_test, (len(X_seg_test), -1))

print(X_seg_train_2D .shape)
print(X_seg_test_2D .shape)

print(X_seg_train.shape)
print(X_seg_test.shape)
print(Y.shape)

from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_train_encoded = label_encoder.fit_transform(Y_train)

# Création de l'estimateur XGBoost
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

# Entraînement du modèle
model.fit(X_seg_train_2D, Y_train_encoded)

# Prédiction sur les données de test
Y_pred_encoded = model.predict(X_seg_test_2D)

# Convertir les prédictions en format original
Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

# Affichage de la matrice de confusion
pd.crosstab(Y_test, Y_pred_xg)

from sklearn.metrics import classification_report

# Affichage du rapport de classification
print(classification_report(Y_test, Y_pred_xg))

"""##4. Validation Hold-Out"""

#Random Forest

"""### Random Forest"""

# Importation des bibliothèques nécessaires
from  sklearn.metrics import accuracy_score
from sklearn import ensemble
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Spécification du nombre d'itérations pour la validation Hold-Out
num_hold= 10
# Initialisation d'une liste vide pour stocker les précisions à chaque itération
accuracies= [ ]

# Redimensionnement de X en 2D pour l'adapter au modèle
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)

# Boucle sur le nombre d'itérations de validation Hold-Out
for i in range(num_hold):
    # Séparation des données en ensembles d'entraînement et de test
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)

    # Création et entraînement du modèle RandomForestClassifier
    clf = ensemble.RandomForestClassifier(n_jobs=-1)
    clf.fit(X_train, Y_train)

    # Prédiction des classes pour l'ensemble de test
    Y_pred_r = clf.predict(X_test)

    # Calcul de la précision pour cette itération
    accuracy =accuracy_score(Y_test, Y_pred_r )
    # Ajout de la précision à la liste des précisions
    accuracies.append(accuracy)

    # Affichage de la précision pour cette itération
    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de la précision à travers les itérations
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
# Ajout d'une ligne horizontale représentant la précision moyenne sur toutes les itérations
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')
# Ajout des labels pour les axes et le titre du graphique
plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
# Affichage du graphique
plt.show()

# Affichage des précisions pour chaque itération
print(f" Accuracy avec la validation Hold-Out: {accuracies}")

# Affichage de la précision moyenne sur toutes les itérations
print(f" Accuracy avec la validation Hold-Out: {np.mean(accuracies)}")

#SVM

"""### SVM"""

# Importation des bibliothèques nécessaires
from  sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

num_hold= 10 # Définition du nombre d'itérations pour la validation Hold-Out
accuracies= [ ] # Initialisation d'une liste vide pour stocker les scores d'accuracy
# Redimensionnement de X en 2D pour s'adapter au modèle SVM
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)


# Boucle sur le nombre d'itérations de validation Hold-Out
for i in range(num_hold):
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)
     # Création et entraînement du modèle SVC
    cls = SVC(kernel='linear', decision_function_shape='ovo')
    cls.fit(X_train, Y_train)
    Y_pred_s = cls.predict(X_test) # Prédiction des classes pour l'ensemble de test

    accuracy =accuracy_score(Y_test, Y_pred_s ) # Calcul de l'accuracy pour cette itération
    accuracies.append(accuracy) # Ajout de l'accuracy à la liste des accuracies

    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de l'accuracy
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')

plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
plt.legend()
plt.show()



print(f" Accuracy avec la validation Hold-Out: {accuracies}")
print(f" Accuracy moyenne avec la validation Hold-Out: {np.mean(accuracies)}")

#3H pour 10 itérations

"""### XGBoost"""

from  sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np

num_hold= 10
accuracies= [ ]
X2D = np.reshape(X,(len(X),-1))
print('shape de X2D:', X2D.shape)

for i in range(num_hold):
    X_train, X_test, Y_train, Y_test = train_test_split(X2D,Y,test_size=0.2)

    # Convertir les classes en format numérique
    label_encoder = LabelEncoder()
    Y_train_encoded = label_encoder.fit_transform(Y_train)

    # Création de l'estimateur XGBoost
    model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_train_encoded)))

    # Entraînement du modèle
    model.fit(X_train, Y_train_encoded)

    # Prédiction sur les données de test
    Y_pred_encoded = model.predict(X_test)

    # Convertir les prédictions en format original
    Y_pred_xg= label_encoder.inverse_transform(Y_pred_encoded)

    accuracy =accuracy_score(Y_test, Y_pred_xg)
    accuracies.append(accuracy)

    print(f"Iteration {i+1}, Accuracy avec la validation Hold-Out: {accuracy}")

# Visualisation de l'évolution de l'accuracy
plt.plot(range(1, num_hold+1), accuracies, marker='o', linestyle= '-', color='blue', label= 'Accuracy par Hold out')
plt.axhline(y= np.mean(accuracies),  linestyle= '--', color='red', label= 'Moyenne des accuracies')

plt.xlabel('Itérations')
plt.ylabel('Accuracy')
plt.title("Évolution de l'accuracy avec la validation Hold-Out")
plt.legend()
plt.show()


print(f" Accuracy avec la validation Hold-Out: {accuracies}")
print(f" Accuracy moyenne avec la validation Hold-Out: {np.mean(accuracies)}")


#15 h et 36 min pour 9 itérartions et plante
#5h 28 min avec plus d'unités
"""shape de X2D: (17204, 30000)
Iteration 1, Accuracy avec la validation Hold-Out: 0.9154315605928509
Iteration 2, Accuracy avec la validation Hold-Out: 0.9067131647776809
Iteration 3, Accuracy avec la validation Hold-Out: 0.8959604766056379
Iteration 4, Accuracy avec la validation Hold-Out: 0.9038070328392909
Iteration 5, Accuracy avec la validation Hold-Out: 0.8936355710549259
Iteration 6, Accuracy avec la validation Hold-Out: 0.902353966870096
Iteration 7, Accuracy avec la validation Hold-Out: 0.9102005231037489
Iteration 8, Accuracy avec la validation Hold-Out: 0.9084568439407149
Iteration 9, Accuracy avec la validation Hold-Out: 0.9072943911653589
Iteration 10, Accuracy avec la validation Hold-Out: 0.8977041557686719

 Accuracy avec la validation Hold-Out: [0.9154315605928509, 0.9067131647776809, 0.8959604766056379, 0.9038070328392909, 0.8936355710549259, 0.902353966870096, 0.9102005231037489, 0.9084568439407149, 0.9072943911653589, 0.8977041557686719]
 Accuracy moyenne avec la validation Hold-Out: 0.9041557686718976 """

"""## Validation k-fold

Randome Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, KFold
import numpy as np

# Remodeler X en une matrice 2D
# K-Fold

"""n_samples = X.shape[0]
n_features = np.prod(X.shape[1:])
X_2D = X.reshape(n_samples, n_features)"""

# Redimensionnement de nos données en 2 dimensions si nécessaire
X2D = np.reshape(X,(len(X),-1))

# Définition du modèle
model = RandomForestClassifier(n_jobs=-1)

# Définition du K-Fold Cross Validation
kfold = KFold(n_splits=10, shuffle=True)

# Calcul des scores via cross validation
scores_kfold = cross_val_score(model, X2D, Y, cv=kfold)

print(f"Scores avec K-Fold Cross Validation: {scores_kfold}")
print(f"Moyenne des scores K-Fold: {np.mean(scores_kfold)}")

"""XGBoost

import xgboost as xgb
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Redimensionnement de nos données en 2 dimensions si nécessaire
X2D = np.reshape(X,(len(X),-1))

# Convertir les classes en format numérique
label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)

# Définition du modèle
model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(Y_encoded)), n_jobs=-1)

# Définition du K-Fold Cross Validation
kfold = KFold(n_splits=4, shuffle=True)

# Calcul des scores via cross validation
scores_kfold = cross_val_score(model, X2D, Y_encoded, cv=kfold)

print(f"Scores avec K-Fold Cross Validation: {scores_kfold}")
print(f"Moyenne des scores K-Fold: {np.mean(scores_kfold)}")

# nécessite plusieurs RAM

# IV. Deep Learning

##1. VGG16

Génération d'images à partir d'un répertoire d'images d'enTRAINement et de TEST
"""

def DidDataGen(directory_train, directory_test, target_size = (100,100), batch_size = 32, shear_range = 0.2, zoom_range = 0.2,
                rotation_range = 359, horizontal_flip = True, vertical_flip = True):

  from tensorflow.keras.preprocessing.image import ImageDataGenerator
  from tensorflow.keras.applications.vgg16 import preprocess_input
  import os

  train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                    shear_range = shear_range, # plage d'étirement
                                    zoom_range = zoom_range,  # plage d'agrandissement
                                    rotation_range = rotation_range, # plage de rotation en degré
                                    horizontal_flip = horizontal_flip,  #retournement horizontal aléatoire
                                    vertical_flip = vertical_flip,  #retournement vertical aléatoire
                                    )

  test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)

  train_generator = train_datagen.flow_from_directory(directory = directory_train,
                                                            class_mode ="sparse",
                                                            target_size=target_size,
                                                            batch_size=batch_size)

  test_generator = test_datagen.flow_from_directory(directory = directory_test,
                                                          class_mode ="sparse",
                                                          target_size=target_size,
                                                          batch_size=batch_size)

    # compte le nb de sous-dossiers dans directory_train
  n_class=0
  for file in os.listdir(directory_train):
      d = os.path.join(directory_train, file)
      if os.path.isdir(d):
          n_class += 1

  return train_generator, test_generator, n_class

train_generator, test_generator, n_class = DidDataGen(path_DS_train, path_DS_test, target_size = (100,100), batch_size = 32, shear_range = 0.2, zoom_range = 0.2,
                rotation_range = 359, horizontal_flip = True, vertical_flip = True)

"""Création et entrainement de plusieurs modèles VGG16 (à 21 couches) selon différents paramètres :
 * nb de couches defreezées = 4, 12, 21
 * taille du batch = 32 ou 64
 * learning rate évolutif par plateau de val_loss (callback)
"""

def DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 4, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = False):

  from tensorflow.keras.applications.vgg16 import VGG16
  from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D
  from tensorflow.keras.optimizers import Adam
  from tensorflow.keras.models import Model, Sequential
  from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
  import matplotlib.pyplot as plt

  # Modèle VGG16
  base_model = VGG16(weights='imagenet', include_top=False)

  # Freeze toutes les couches du VGG16 sauf les n dernières (si n différent de 0)
  for layer in base_model.layers:
    layer.trainable = False

  if n_layers_trainable != 0:
    for layer in base_model.layers[- n_layers_trainable:]:
      layer.trainable = True

  # Callbacks
  early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0.01, mode ='min', patience = 6, verbose = 1)
  reduce_learning_rate = ReduceLROnPlateau (monitor = 'val_loss', factor = 0.1, patience = 3, min_delta = 0.01, cooldown = 4, verbose = 1)
  callbacks = [reduce_learning_rate, early_stopping]

  # Construction du modèle
  model = Sequential()
  model.add(base_model)
  model.add(GlobalAveragePooling2D())
  model.add(Dense(1024,activation='relu'))
  model.add(Dropout(rate=0.2))
  model.add(Dense(512, activation='relu'))
  model.add(Dropout(rate=0.2))
  model.add(Dense(n_class, activation='softmax'))

  model.compile(optimizer=Adam(learning_rate= learning_rate), loss='sparse_categorical_crossentropy', metrics=['acc'])

  # Entrainement du modèle

  print("Entrainement du modèle")
  history = model.fit(train_generator,
                      epochs = nb_epochs,
                      steps_per_epoch = train_generator.samples//batch_size,
                      validation_data = test_generator,
                      validation_steps = test_generator.samples//batch_size,
                      callbacks = callbacks)


  # Courbe de la fonction de coût et de précision en fonction de l'epoch

  print("\nCourbes de perte et de précision pour VGG16 avec les paramètres:\n # couches entrainées:", n_layers_trainable, "\n # learning rate init:", learning_rate,"\n # epochs            :",nb_epochs,"\n # batch size        :", batch_size)

  train_loss = history.history["loss"]
  val_loss = history.history["val_loss"]
  train_acc = history.history["acc"]
  val_acc = history.history["val_acc"]

  plt.figure(figsize = (20, 7))

  plt.subplot(121)
  plt.plot(train_loss)
  plt.plot(val_loss)
  plt.title('Model loss per epoch')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='right')


  plt.subplot(122)
  plt.plot(train_acc)
  plt.plot(val_acc)
  plt.title('Model accuracy per epoch')
  plt.ylabel('acc')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='right')

  plt.show();

  print("\n", model.summary())

    # Evaluation du modèle (un peu long donc désactivée par défaut)
  if model_eval == True:
    print("\nEvaluation du modèle sur l'ensemble de test augmenté par génération de données:")
    score = model.evaluate(test_generator)
    return model, history, score
  else:
    return model, history


def DidSave(variable, fichier):
  import pickle
  f = open(fichier,"wb")
  pickle.dump(variable,f)
  f.close()

def DidLoad(fichier):
  import pickle
  f = open(fichier,"rb")
  variable = pickle.load(f)
  f.close()
  return variable

model_4_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 4, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)
DidSave(model_4_64, '/content/drive/MyDrive/BD/model_4_layers_64_batch')

model_0_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 0, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)
DidSave(model_0_64, '/content/drive/MyDrive/BD/model_0_layers_64_batch')

model_12_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 12, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)

model_21_32,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 21, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = True)

model_21_64,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 21, learning_rate = 1e-4, nb_epochs = 30, batch_size = 64, model_eval = True)

"""Sauvegarde des modèles (en cas de plantage de l'environnement pour saturation de RAM)"""



#DidSave(model_12_64, '/content/drive/MyDrive/BD/model_12_layers_64_batch')

#DidSave(model_21_64, '/content/drive/MyDrive/BD/model_16_layers_64_batch')

"""Feature Extraction en sortie des couches 2 et 5 puis entrainement de différents modèles de classification (Arbre de décision, SVM, Random Forest et XGBoost)"""

def DidFeatureExtractionClassification(X, Y, model, test_size = 0.2, layers_output = 2 , xgb = False):

  # extraction des features de la couche 2 et entrainement d'un modèle de classification
  from sklearn.model_selection import train_test_split
  from tensorflow.keras.models import Model
  from tensorflow.keras.applications.vgg16 import preprocess_input

  X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size)

  intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[layers_output].output)
  print('Train feature extraction')
  X_train_features = intermediate_layer_model.predict(preprocess_input(X_train))
  print('Test feature extraction')
  X_test_features = intermediate_layer_model.predict(preprocess_input(X_test))

  # decision tree
  from sklearn.tree import DecisionTreeClassifier
  clf = DecisionTreeClassifier()
  clf.fit(X_train_features, Y_train)
  print('score de classification avec Arbre de décision :', clf.score(X_test_features, Y_test))

  # SVM
  from sklearn.svm import SVC
  clf = SVC()
  clf.fit(X_train_features, Y_train)
  print('score de classification avec SVM               :', clf.score(X_test_features, Y_test))

  # Random Forest
  from sklearn import ensemble
  clf = ensemble.RandomForestClassifier(n_jobs=-1)
  clf.fit(X_train_features, Y_train)
  print('score de classification avec Random Forest     :', clf.score(X_test_features, Y_test))

  # XGBoost (un peu long donc désactivée par défaut)
  if xgb == True:

    import xgboost as xgb
    clf = xgb.XGBClassifier()

    from sklearn import preprocessing
    le = preprocessing.LabelEncoder()
    le.fit(Y_train)
    Y_train_enc = le.transform(Y_train)
    Y_test_enc = le.transform(Y_test)

    clf.fit(X_train_features, Y_train_enc)
    print('score de classification avec XGBoost           :', clf.score(X_test_features, Y_test_enc))

  return X_train_features, X_test_features

#X,Y = DidPreprocessing(path_DS, img_width = 100, img_height = 100, drop_duplicates = True)
#DidSave(X,'/content/drive/MyDrive/BD/variable_X')
#DidSave(Y,'/content/drive/MyDrive/BD/variable_Y')

X = DidLoad('/content/drive/MyDrive/BD/variable_X')
Y = DidLoad('/content/drive/MyDrive/BD/variable_Y')

for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_21_32, layers_output = i , xgb = True)

for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_12_64, layers_output = i , xgb = True)

for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_21_64, layers_output = i , xgb = True)

model_12_32,_,_ = DidVGG16(train_generator, test_generator, n_class, n_layers_trainable = 12, learning_rate = 1e-4, nb_epochs = 30, batch_size = 32, model_eval = True)
DidSave(model_12_32, '/content/drive/MyDrive/BD/model_12_layers_32_batch')
for i in [2,5]:
  _,_ = DidFeatureExtractionClassification(X, Y, model_12_32, layers_output = i , xgb = True)

"""Scores de référence des 4 modèles sur les données sans feature extraction"""

# en utilisant les ensembles d'entrainement et de test standardisés et réduits à 2 dimensions

# decision tree
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, Y_train)
print('score de classification avec Arbre de décision :', clf.score(X_test, Y_test))

# SVM
from sklearn.svm import SVC
clf = SVC()
clf.fit(X_train, Y_train)
print('score de classification avec SVM               :', clf.score(X_test, Y_test))

# Random Forest
from sklearn import ensemble
clf = ensemble.RandomForestClassifier(n_jobs=-1)
clf.fit(X_train, Y_train)
print('score de classification avec Random Forest     :', clf.score(X_test, Y_test))

# XGBoost
import xgboost as xgb
clf = xgb.XGBClassifier()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(Y_train)
Y_train_enc = le.transform(Y_train)
Y_test_enc = le.transform(Y_test)

clf.fit(X_train, Y_train_enc)
print('score de classification avec XGBoost           :', clf.score(X_test, Y_test_enc))

"""##2. ResNet50"""

#création des datasets d'entrainement et de test à partir des fichiers jpg

import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt
import time

#import cv2
import seaborn as sns

import pandas as pd
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras as K

from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras import callbacks

from tensorflow.keras.applications.vgg16 import preprocess_input

train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                   shear_range = 0.2, # plage d'étirement
                                   zoom_range = 0.2,  # plage d'agrandissement
                                   rotation_range = 359, # plage de rotation en degré
                                   horizontal_flip = True,  #retournement horizontal aléatoire
                                   vertical_flip = True,  #retournement vertical aléatoire
                                   )

test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)

train_generator = train_datagen.flow_from_directory(directory=path_DS_train,
                                                           class_mode ="sparse",
                                                           target_size=(100,100),
                                                           batch_size=32)

test_generator = test_datagen.flow_from_directory(directory=path_DS_test,
                                                  class_mode ="sparse",
                                                  target_size=(100,100),
                                                  batch_size=32)

print("modele2 : C2 143 - 224x224 - lr 1e-4")

inputs = K.Input(shape=(224, 224, 3))

n_class = 9

resnet2 = K.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)

for layer in resnet2.layers[:143]:
    layer.trainable = False

model2 = K.models.Sequential()
model2.add(K.layers.Lambda(lambda x: tf.image.resize(x, (224, 224))))
model2.add(resnet2)

model2.add(Flatten())
model2.add(Dense(512, activation='relu'))
model2.add(Dense(10, activation='softmax'))

ces = callbacks.EarlyStopping(monitor='accuracy', patience=4, mode='max', restore_best_weights=True)
crop = callbacks.ReduceLROnPlateau(monitor='accuracy', patience=2, verbose=2, mode='max')

model2.compile(loss='sparse_categorical_crossentropy',
              optimizer=K.optimizers.legacy.RMSprop(1e-4),
              metrics=['accuracy'])

history2 = model2.fit(train_generator,
                    batch_size=32,
                    epochs=100,
                    verbose=1,
                    validation_data=test_generator,
                    shuffle=True,
                    callbacks=[ces, crop])

train_loss = history2.history["loss"]
val_loss = history2.history["val_loss"]
train_acc = history2.history["accuracy"]
val_acc = history2.history["val_accuracy"]

plt.figure(figsize = (20, 8))

plt.subplot(121)
plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Model loss per epoch - Modele2')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='right')


plt.subplot(122)
plt.plot(train_acc)
plt.plot(val_acc)
plt.title('Model accuracy per epoch - Modele2')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='right')

plt.show();

model2.summary()

#permet de charger un modèle et de sortir son évaluation
model2 = tf.keras.models.load_model('/content/drive/MyDrive/BD/vf_model2')
model2.evaluate(test_generator)

#on donne le chemin d'une image enregistrée sur pc
#et il ressort la classe prédite avec la probabilité
def test_from_local(chemin, model, test_generator):
    from tensorflow.keras.preprocessing import image
    import matplotlib.image as mpimg
    import numpy as np
    #met les images à la bonne dimension
    img = image.load_img(chemin, target_size=(100, 100))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    #obtenir l'indice de la classe la plus probable
    arg = int(tf.argmax(model.predict(x), axis=1).numpy())
    #obtenir la liste des noms des classes
    class_names = list(test_generator.class_indices.keys())
    #valeur de la probabilité que l'image appartienne à la classe
    proba = model.predict(x)[0][arg]

    return print("Cette image a", proba*100, "% de chance d'être un", class_names[arg])

#quelques tests de la fonction précdente (qui fonctionne bien)
test_from_local("/content/drive/MyDrive/BD/a_tester_eosinophil.jpg", model2, test_generator)

#on donne le chemin d'une image enregistrée sur pc
#et il ressort la classe prédite avec la probabilité
def test_from_url(image_url, model, test_generator):
    from tensorflow.keras.preprocessing import image
    import matplotlib.image as mpimg
    import numpy as np
    import urllib.request
    import io

    with urllib.request.urlopen(image_url) as url:
        image_data = url.read()

    img = image.load_img(io.BytesIO(image_data), target_size=(100, 100))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    arg = int(tf.argmax(model.predict(x), axis=1).numpy())

    class_names = list(test_generator.class_indices.keys())
    proba = model.predict(x)[0][arg]

    return print("Cette image a", proba*100, "% de chance d'être un", class_names[arg])

#quelques tests de la fonction précdente (qui fonctionne bien)
test_from_url('http://bioimage.free.fr/hem_image/hem_img/pb32l.jpg', model2, test_generator)

"""##3. DenseNet121"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input , Dense
from tensorflow.keras.models import Model

import tensorflow as tf

#Rechargement des données sauvegardées

X_dnet121 = X_reload
Y_dnet121 = Y_reload

Y_dnet121



#Fonction permettant de préparer les données pour petre conforme à l'entrée du DenseNet121

from sklearn.preprocessing import LabelEncoder
import tensorflow as tf

encoder = LabelEncoder()
encoder.fit(Y_dnet121)
saved_labels = encoder.inverse_transform([i for i in range(9)])

def preprocess_data(X, Y):

    X = np.float32(X)
    Y = np.array(Y)
    """
    function that pre-processes th dataset as per
    densenet model requirements for input images
    labels are one-hot encoded
    """
    encoder = LabelEncoder()
    X = tf.keras.applications.densenet.preprocess_input(X)
    Y = encoder.fit_transform(Y)
    Y = tf.keras.utils.to_categorical(Y , 9)
    return X, Y

from sklearn.model_selection import train_test_split

X_dnet121_train_base , X_dnet121_test_base , Y_dnet121_train_base , Y_dnet121_test_base = train_test_split( X_dnet121 , Y_dnet121 , test_size = 0.2 )

#Preprocessing train data pour le densenet121

X_dnet121_train , Y_dnet121_train = preprocess_data( X_dnet121_train_base , Y_dnet121_train_base )

#Preprocessing test data pour le densenet121

X_dnet121_test , Y_dnet121_test = preprocess_data( X_dnet121_test_base , Y_dnet121_test_base )

#Implémentation du DenseNet121 et gel des 150 premières couches

base_densenet121 = tf.keras.applications.DenseNet121( include_top = False , weights = 'imagenet' )

#Gel des 150 premières couches

for layer in base_densenet121.layers[ :149 ] :
  layer.trainable = False
for layer in base_densenet121.layers[ 149: ] :
  layer.trainable = True

#Consutrction du modèle

model_densenet121 = tf.keras.models.Sequential()

#Mise en forme des données pour le DenseNet121

model_densenet121.add( tf.keras.layers.Lambda( lambda x : tf.keras.backend.resize_images( x ,
                                                              height_factor = 1 ,
                                                              width_factor = 1 ,
                                                              data_format = 'channels_last' )  ) )

#Construction du modèle (classifieur)

kernel_init = 'normal'

model_densenet121.add( base_densenet121 )
model_densenet121.add( tf.keras.layers.Flatten() )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(512, activation='relu',kernel_initializer=kernel_init) )
model_densenet121.add( tf.keras.layers.Dropout(0.7) )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(128, activation='relu',kernel_initializer=kernel_init))
model_densenet121.add( tf.keras.layers.Dropout(0.5) )
model_densenet121.add( tf.keras.layers.BatchNormalization() )
model_densenet121.add( tf.keras.layers.Dense(64, activation='relu',kernel_initializer=kernel_init) )
model_densenet121.add( tf.keras.layers.Dropout(0.3) )
model_densenet121.add( tf.keras.layers.Dense(9, activation='softmax',kernel_initializer=kernel_init) )

#Callbacks

from tensorflow.keras import callbacks

CB = []

CB.append(callbacks.ModelCheckpoint( filepath='CB_best' , monitor='val_accuracy', save_best_only=True ))

CB.append( callbacks.EarlyStopping( monitor='val_accuracy', min_delta = 0.01 , patience = 5 ) )

CB.append( callbacks.ReduceLROnPlateau (monitor = 'val_loss', factor = 0.1, patience = 3, min_delta = 0.01, cooldown = 4, verbose = 1) )

#Compile

optimizer = 'Adam'

model_densenet121.compile( optimizer = optimizer, loss= 'categorical_crossentropy' , metrics=['accuracy'] )

#Train

history_densenet121 = model_densenet121.fit( x = X_dnet121_train , y = Y_dnet121_train ,
                                 batch_size = 32 , epochs = 30 ,
                                 callbacks = CB , validation_data = ( X_dnet121_test , Y_dnet121_test ) ,
                                  verbose = True )

#Enregistrement du modèle

import pickle

pickle_out = open('/content/drive/MyDrive/BD/model_densenet121.pckl', 'wb')
pickle.dump(model_densenet121, pickle_out)
pickle_out.close()

#Enregistrement de l'historique

pickle_out = open('/content/drive/MyDrive/BD/history_densenet121.pckl', 'wb')
pickle.dump(history_densenet121, pickle_out)
pickle_out.close()

model_densenet121.summary()

#Affichage de l'évolution de l'accuracy de la loss

train_acc = history_densenet121.history['accuracy']

val_acc = history_densenet121.history['val_accuracy']

plt.plot(train_acc , label = "train accuracy")
plt.plot(val_acc , label = "test accuracy")
plt.title('DenseNet121 : Accuracy')
plt.legend( loc = 'lower right' )
plt.grid(True)
plt.show();

train_loss = history_densenet121.history['loss']

val_loss = history_densenet121.history['val_loss']

plt.plot(train_loss , label = "train loss")
plt.plot(val_loss , label = "test loss")
plt.title('DenseNet121 : Loss')
plt.legend( loc = 'upper right' )
plt.grid(True)
plt.show();

#Prédictions du modèle

probs_pred_densenet121 = model_densenet121.predict( X_dnet121_test )

Y_pred_densenet121 = np.argmax( probs_pred_densenet121 , axis = 1 )

Y_test_densenet121_sparse = np.argmax( Y_dnet121_test , axis = 1 )

from sklearn.metrics import confusion_matrix

conf_matrix_densenet121 = confusion_matrix( Y_test_densenet121_sparse , Y_pred_densenet121 )

print(conf_matrix_densenet121)

from sklearn.metrics import classification_report

class_report_densenet121 = classification_report( Y_test_densenet121_sparse , Y_pred_densenet121 )

print( class_report_densenet121 )

#Implémentation du DenseNet121 sans geler aucun calque

base_densenet121_nf = tf.keras.applications.DenseNet121( include_top = False , weights = 'imagenet' )

model_densenet121_nf = tf.keras.models.Sequential()

model_densenet121_nf.add( tf.keras.layers.Lambda( lambda x : tf.keras.backend.resize_images( x ,
                                                              height_factor = 1 ,
                                                              width_factor = 1 ,
                                                              data_format = 'channels_last' )  ) )

#Construction du modèle (même classifieur)

kernel_init = 'normal'

model_densenet121_nf.add( base_densenet121_nf )
model_densenet121_nf.add( tf.keras.layers.Flatten() )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(512, activation='relu',kernel_initializer=kernel_init) )
model_densenet121_nf.add( tf.keras.layers.Dropout(0.7) )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(128, activation='relu',kernel_initializer=kernel_init))
model_densenet121_nf.add( tf.keras.layers.Dropout(0.5) )
model_densenet121_nf.add( tf.keras.layers.BatchNormalization() )
model_densenet121_nf.add( tf.keras.layers.Dense(64, activation='relu',kernel_initializer=kernel_init) )
model_densenet121_nf.add( tf.keras.layers.Dropout(0.3) )
model_densenet121_nf.add( tf.keras.layers.Dense(9, activation='softmax',kernel_initializer=kernel_init) )

#Compile

optimizer = 'Adam'

model_densenet121_nf.compile( optimizer = optimizer, loss= 'categorical_crossentropy' , metrics=['accuracy'] )

#Train

history_densenet121_nf = model_densenet121_nf.fit( x = X_dnet121_train , y = Y_dnet121_train ,
                                 batch_size = 32 , epochs = 30 ,
                                 callbacks = CB , validation_data = ( X_dnet121_test , Y_dnet121_test ) ,
                                  verbose = True )

#Enregistrement du modèle et de l'historique

import pickle

pickle_out = open('/content/drive/MyDrive/BD/model_densenet121_nf.pckl', 'wb')
pickle.dump(model_densenet121_nf, pickle_out)
pickle_out.close()

pickle_in = open('/content/drive/MyDrive/BD/model_densenet121_nf.pckl' , 'rb')
model_densenet121_nf = pickle.load(pickle_in)

pickle_out = open('/content/drive/MyDrive/BD/history_densenet121_nf.pckl', 'wb')
pickle.dump(history_densenet121_nf, pickle_out)
pickle_out.close()

model_densenet121_nf.summary()

#Courbes de l'accurarcy et de la loss

train_acc_nf = history_densenet121_nf.history['accuracy']

val_acc_nf = history_densenet121_nf.history['val_accuracy']

plt.plot(train_acc_nf , label = "train accuracy")
plt.plot(val_acc_nf , label = "test accuracy")
plt.title('DenseNet121 : Accuracy')
plt.legend( loc = 'lower right' )
plt.grid(True)
plt.show();

train_loss_nf = history_densenet121_nf.history['loss']

val_loss_nf = history_densenet121_nf.history['val_loss']

plt.plot(train_loss_nf , label = "train loss")
plt.plot(val_loss_nf , label = "test loss")
plt.title('DenseNet121 : Loss')
plt.legend( loc = 'upper right' )
plt.grid(True)
plt.show();

#Prédictions du modèle

probs_pred_densenet121_nf = model_densenet121_nf.predict( X_dnet121_test )

Y_pred_densenet121_nf = np.argmax( probs_pred_densenet121_nf , axis = 1 )

Y_test_densenet121_sparse = np.argmax( Y_dnet121_test , axis = 1 )

#Matrice de confusion et rapports de classification

from sklearn.metrics import confusion_matrix

conf_matrix_densenet121_nf = confusion_matrix( Y_test_densenet121_sparse , Y_pred_densenet121_nf )

print(conf_matrix_densenet121_nf)

from sklearn.metrics import classification_report

class_report_densenet121_nf = classification_report( Y_test_densenet121_sparse , Y_pred_densenet121_nf )

print( class_report_densenet121_nf )

from imblearn.metrics import classification_report_imbalanced

print(classification_report_imbalanced( Y_test_densenet121_sparse , Y_pred_densenet121_nf ))

#Génération et classification d'images du test set

import random

plt.figure( figsize = ( 10 , 6 ) )

for i in range(8) :

  k = random.randint( 0 , len(X_dnet121_test_base) )
  plt.subplot( 2 , 4 , i+1 )
  plt.imshow( X_dnet121_test_base[k] )
  plt.axis(False)
  plt.title( saved_labels[Y_test_densenet121_sparse[k]]
              + '\npredicted as\n'
              + saved_labels[Y_pred_densenet121_nf[k]] )
plt.show();

"""# Conclusion

Si la segmentation n’a pas permis d'améliorer les scores de classification, les scores de précision obtenus avec différents modèles de machine learning (avec et sans oversampling) et de deep learning (avec et sans data generation) sur les cellules saines sont très satisfaisants :
*   Machine Learning : 91% avec le classifier XGBoost
*   Deep Learning : 95 à 98% avec les CNN VGG16, ResNet50 et DenseNet121
*   Feature Extraction : 98 à 99% avec VGG16 puis XGBoost

Différentes pistes d’amélioration des scores des modèles testées pourraient être testées :
*   Machine Learning : le score obtenu par classifieur XGBoost pourrait être amélioré par l’entrainement sur les données rééchantillonnées par Over Sampling.
*   Transfer Learning : les scores obtenus par les réseaux neuronaux convolutifs profonds tels que ResNet50 et DenseNet121 pourraient être optimisés par la génération d’images ainsi que la classification par un modèle de machine learning après l’extraction des features.
*   Feature Extraction : les scores obtenus après entrainement de classifieurs sur les features extraites en sortie des couches intermédiaires du VGG16 pourraient être optimisés par un réglage des hyperparamètres des modèles de machine learning utilisés.

Pour ateindre l’objectif final du diagnostic certaines pathologies par identification des cellules sanguines anormales, il conviendrait d’entrainer ces modèles sur les jeux de données incluant des cellules anormales et distinguer ainsi les cellules pathologiques après avoir identifié leur type.
"""